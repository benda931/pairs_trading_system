# -*- coding: utf-8 -*-
"""
common/fundamental_loader.py â€” Index/ETF Fundamentals Ingestion Layer (HF-grade)
===============================================================================

××˜×¨×ª ×”××•×“×•×œ
-----------
×©×›×‘×ª **ingestion + × ×™×”×•×œ** ××—×ª ×œ×›×œ ×”×“××˜×” ×”×¤× ×“×•×× ×˜×œ×™ ×‘×¨××ª ××“×“ / ETF / Basket:

- ××“×“×™× ×›×œ×œ×™×™×: SPY, QQQ, IWM, DIA, VTI, VT...
- ××“×“×™ ××–×•×¨×™×: EEM, EFA, EWJ, EWU, VNQ, ×•×›×•'.
- ×¡×§×˜×•×¨×™×: XLF, XLK, XLE, XLY, XLV, XLU, XLI, XLB...
- ××“×“×™ Style / Factor: Value, Growth, Quality, Small, Momentum ×•×›×•' (×× ×ª×•×¡×™×£).

×”××¢×¨×›×ª × ×‘× ×™×ª ×›×š ×©×ª×ª××•×š *×‘×›×œ ××” ×©×ª×›× × ×•*:
- ×˜×•×•×—×™ ×–××Ÿ ××¨×•×›×™× (×¢×“ 10 ×©× ×™× ×•×™×•×ª×¨) ×œ×›×œ ×”××“×“×™×.
- ×›×™×¡×•×™ ×¨×—×‘ "×‘××•×¤×Ÿ ×§×™×¦×•× ×™" ×©×œ ×¤×™×¦'×¨×™× ×¤× ×“×•×× ×˜×œ×™×™×:
  P/E, Forward P/E, P/B, Dividend Yield, Earnings Yield,
  ROE, ROA, ROIC, Net Margin, Operating Margin,
  Revenue/EPS/FCF Growth (3Y/5Y),
  Net Debt / EBITDA, Interest Coverage, Payout Ratio,
  ×•×¢×•×“ ×ª×ª×™-×¤×™×¦'×¨×™× ×©× ×¨×¦×” ×œ×”×•×¡×™×£.

- ×ª××™×›×” ×‘××§×•×¨×•×ª ×“××˜×” ×©×•× ×™× (Providers):
  * Local files (CSV/Parquet) â€“ for backfills / offline research.
  * Tiingo / Financial APIs â€“ ×›×©× ×¨×¦×” ×œ×”×–×™×Ÿ × ×ª×•× ×™× ×××™×ª×™×™×.
  * IBKR fundamentals â€“ ×”×ª×××” ×œ××¢×¨×›×ª ×”××¡×—×¨ ×”×—×™×”.
  * Provider ××•×ª×× ××™×©×™×ª (×œ××©×œ DuckDB ×¤× ×™××™).

- × ×™×”×•×œ Cache ×œ×•×§×œ×™ ××œ×:
  * ×©××™×¨×” ×©×œ ×›×œ symbol ×‘×§×‘×¦×™ Parquet/CSV + ×§×•×‘×¥ meta.json.
  * TTL (time to live) ×œ×¤×™ ×™××™×.
  * ××¤×©×¨×•×ª ×œ-force refresh, partial refresh (incremental), ×•×œ×•×’×™×.

- API ×¤× ×™××™ × ×§×™ ×œ×©××¨ ×”××¢×¨×›×ª (core/index_fundamentals, core/index_clustering ×•×›×•'):
  * load_index_fundamentals(...)
  * load_indices_fundamentals(...)
  * build_fundamentals_panel(...)
  * later: incremental_update_index_fundamentals(...)

××‘× ×” ×”×§×•×‘×¥ (4 ×—×œ×§×™×)
---------------------
×—×œ×§ 1/4 (×›××Ÿ):
    - ×ª×™×¢×•×“ ×›×œ×œ×™.
    - imports, logger.
    - FundamentalsSettings (Pydantic Settings).
    - ×˜×™×¤×•×¡×™× ×‘×¡×™×¡×™×™× (FundamentalFrame / Panel).
    - Provider registry ×‘×¡×™×¡×™ (without actual API calls).
    - ×¢×–×¨×™ path ×•-naming ×œ×¡×™××•×œ×™×.

×—×œ×§ 2/4:
    - I/O ×œ×•×§×œ×™: ×§×¨×™××”/×›×ª×™×‘×” Parquet/CSV.
    - ××˜×-×“××˜×” (meta.json) ×•-JSON safe.
    - Normalize DataFrame (××™× ×“×§×¡ ×–××Ÿ, ×©××•×ª ×¢××•×“×•×ª).
    - ×ª×‘× ×™×ª Provider abstraction (BaseFundamentalsProvider + registry).

×—×œ×§ 3/4:
    - ×¤×•× ×§×¦×™×•×ª public:
        * load_index_fundamentals
        * load_index_fundamentals_cached
        * load_indices_fundamentals
        * build_fundamentals_panel
    - ×ª××™×›×” ×‘-fields, ×˜×•×•×—×™ ×–××Ÿ, provider override.
    - ×œ×•×’×™× ×•-handling ×—×›× ×©×œ cache vs remote.

×—×œ×§ 4/4:
    - ×”×¨×—×‘×•×ª ××ª×§×“××•×ª:
        * incremental_refresh (append-only ××”-provider).
        * diagnostics / health-check (××™ symbol ×—×¡×¨ / ×××—×¨).
        * utilities ×œ-core (×œ××©×œ cast ×œ×¤×•×¨××˜ wide/long).
        * hooks ×œ×©×™×œ×•×‘ ×¢× DuckDB / local DB (××•×¤×¦×™×•× ×œ×™).

×©×™××• ×œ×‘:
--------
×‘×—×œ×§ 1/4 **××™×Ÿ** ×¢×“×™×™×Ÿ ×§×¨×™××ª API ×××™×ª×™×ª â€“ ×–×” ×™×’×™×¢ ×‘×—×œ×§×™× ×”×‘××™× ×›Ö¾Base Provider + ××™××•×©×™×.
×›×‘×¨ ×¢×›×©×™×• ×”××•×“×•×œ ×‘× ×•×™ ×›×š ×©×§×œ ×œ×”×¨×—×™×‘×• ××‘×œ×™ ×œ×©×‘×•×¨ ××ª ×©××¨ ×”××¢×¨×›×ª.
"""

from __future__ import annotations

# =========================
# Imports
# =========================

import logging
from dataclasses import dataclass
from datetime import date, datetime, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Iterable, Mapping, MutableMapping, Sequence

import pandas as pd
from pydantic import Field
try:  # Pydantic v2 style (××•×¢×“×£)
    from pydantic_settings import BaseSettings
except ImportError:  # pragma: no cover - fallback ×œ×¡×‘×™×‘×•×ª ×‘×œ×™ pydantic_settings
    from pydantic import BaseSettings  # type: ignore

from common.json_safe import make_json_safe, json_default as _json_default
from common.helpers import get_logger
# ×× ×™×© ×œ×š common/typing_compat ×¢× StrPath/Sequence ×ª×•×›×œ ×œ×™×™×‘× ×‘×¢×ª×™×“:
# from common.typing_compat import StrPath  # ×œ× ×—×•×‘×” ×‘×©×œ×‘ ×–×”


# =========================
# Logger
# =========================

logger = get_logger("common.fundamental_loader")


# =========================
# Settings
# =========================


class FundamentalsSettings(BaseSettings):
    """
    ×”×’×“×¨×•×ª ×œ×˜×¢×™× ×”/×©××™×¨×”/×¨×¢× ×•×Ÿ ×©×œ ×“××˜×” ×¤× ×“×•×× ×˜×œ×™ ×œ××“×“×™×/ETF.

    × ×™×ª×Ÿ ×œ×©×œ×•×˜ ×“×¨×š ××©×ª× ×™ ×¡×‘×™×‘×” / .env, ×œ×“×•×’××”:
    ------------------------------------------------
    FUNDAMENTALS_DATA_DIR=./data/fundamentals
    FUNDAMENTALS_CACHE_TTL_DAYS=7
    FUNDAMENTALS_DEFAULT_PROVIDER=tiingo
    FUNDAMENTALS_ALWAYS_REFRESH=false
    FUNDAMENTALS_ALLOW_PARTIAL=true
    FUNDAMENTALS_PANEL_FREQ=D
    """

    # ğŸ“‚ ×ª×™×§×™×™×ª ×‘×¡×™×¡ ×œ×“××˜×” ×¤× ×“×•×× ×˜×œ×™
    FUNDAMENTALS_DATA_DIR: Path = Field(
        default=Path("data") / "fundamentals",
        env="FUNDAMENTALS_DATA_DIR",
    )

    # ğŸ•’ TTL â€“ ×›××” ×–××Ÿ (×‘×™××™×) ×§×•×‘×¥ × ×—×©×‘ "×˜×¨×™"
    FUNDAMENTALS_CACHE_TTL_DAYS: int = Field(
        default=7,
        env="FUNDAMENTALS_CACHE_TTL_DAYS",
    )

    # ğŸ”Œ ×¡×¤×§ ×‘×¨×™×¨×ª ××—×“×œ (logical name) â€“ ×™×•×’×“×¨ ×‘-registry:
    # ×œ×“×•×’××”: "tiingo", "ibkr", "local_duckdb", "csv_only"
    FUNDAMENTALS_DEFAULT_PROVIDER: str = Field(
        default="tiingo",
        env="FUNDAMENTALS_DEFAULT_PROVIDER",
    )

    # ğŸ” ×”×× ×ª××™×“ ×œ× ×¡×•×ª ×¨×¢× ×•×Ÿ ××”×¡×¤×§, ×’× ×× ×”×§×•×‘×¥ ×˜×¨×™
    FUNDAMENTALS_ALWAYS_REFRESH: bool = Field(
        default=False,
        env="FUNDAMENTALS_ALWAYS_REFRESH",
    )

    # âš ï¸ ×”×× ××•×ª×¨ partial data (×œ××©×œ ×—×¡×¨×™× ×—×œ×§ ××”×©×“×•×ª) ××• ×©× ×“×¨×•×© full schema
    FUNDAMENTALS_ALLOW_PARTIAL: bool = Field(
        default=True,
        env="FUNDAMENTALS_ALLOW_PARTIAL",
    )

    # ğŸ§± ×ª×“×™×¨×•×ª ×‘×¨×™×¨×ª ××—×“×œ ×œ×¤×× ×œ (resample ×× ×¦×¨×™×š): "D", "W", "M", "Q"
    FUNDAMENTALS_PANEL_FREQ: str = Field(
        default="M",
        env="FUNDAMENTALS_PANEL_FREQ",
    )

    model_config = {
        "env_file": ".env",
        "case_sensitive": False,
        "extra": "ignore",
    }


SETTINGS = FundamentalsSettings()


# =========================
# Typing aliases
# =========================

FundamentalFrame = pd.DataFrame  # DataFrame ×¢×‘×•×¨ symbol ×‘×•×“×“
FundamentalPanel = pd.DataFrame  # MultiIndex (date, symbol) ×¢×‘×•×¨ universe


@dataclass
class FundamentalMeta:
    """
    Metadata ×‘×¡×™×¡×™ ×¢×‘×•×¨ ×§×•×‘×¥ ×¤× ×“×•×× ×˜×œ ××—×“ (per symbol).

    × ×©××¨ ×›-JSON ×œ×¦×“ ×§×•×‘×¥ ×”×“××˜×”, ×œ××©×œ:
        data/fundamentals/SPY.meta.json
    """

    symbol: str
    source: str
    last_refresh: datetime
    n_rows: int
    n_cols: int
    fields: Sequence[str]

    @classmethod
    def from_dict(cls, data: Mapping[str, Any]) -> "FundamentalMeta":
        last_refresh_raw = data.get("last_refresh")
        if isinstance(last_refresh_raw, str):
            try:
                last_refresh_val = datetime.fromisoformat(last_refresh_raw)
            except ValueError:
                last_refresh_val = datetime.min
        elif isinstance(last_refresh_raw, datetime):
            last_refresh_val = last_refresh_raw
        else:
            last_refresh_val = datetime.min

        return cls(
            symbol=str(data.get("symbol") or ""),
            source=str(data.get("source") or ""),
            last_refresh=last_refresh_val,
            n_rows=int(data.get("n_rows") or 0),
            n_cols=int(data.get("n_cols") or 0),
            fields=list(data.get("fields") or []),
        )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "symbol": self.symbol,
            "source": self.source,
            "last_refresh": self.last_refresh.isoformat(),
            "n_rows": int(self.n_rows),
            "n_cols": int(self.n_cols),
            "fields": list(self.fields),
        }


# =========================
# Provider registry (Skeleton)
# =========================

class BaseFundamentalsProvider:
    """
    ×‘×¡×™×¡ ××•×¤×©×˜ ×œ×›×œ ×¡×¤×§×™ ×”×“××˜×” ×”×¤× ×“×•×× ×˜×œ×™×™×.

    ×”×¨×¢×™×•×Ÿ:
    -------
    - ×›×œ provider (Tiingo/IBKR/CSV/DuckDB) ×™×™×¨×© ××”××—×œ×§×” ×”×–××ª.
    - ×”××•×“×•×œ fundamental_loader ×™×“×‘×¨ ×ª××™×“ ×¢× API ××—×™×“:
        .fetch(symbol, start, end, fields)

    ×”×—×•×–×”:
    -------
    - ××—×–×™×¨×™× DataFrame ×¢×:
        index: DatetimeIndex (×ª××¨×™×š ×“×•"×— / period end)
        columns: ×©××•×ª ×¤× ×“×•×× ×˜×œ×™×™× (pe, pb, roe, dividend_yield ×•×›×•').
    - normalization ×©×œ ×©××•×ª ×¢××•×“×•×ª ×•××™× ×“×§×¡ ×™×™×¢×©×” ×‘××•×“×•×œ ×–×” (×œ× ×‘-provider).
    """

    name: str = "base"

    def fetch(
        self,
        symbol: str,
        *,
        start: date | None = None,
        end: date | None = None,
        fields: Sequence[str] | None = None,
    ) -> FundamentalFrame:
        raise NotImplementedError("BaseFundamentalsProvider.fetch must be overridden")


# registry: ×©× ×œ×•×’×™ -> instance ×©×œ provider
_FUNDAMENTALS_PROVIDERS: Dict[str, BaseFundamentalsProvider] = {}


def register_fundamentals_provider(provider: BaseFundamentalsProvider) -> None:
    """
    ×¨×™×©×•× ×¡×¤×§ ×—×“×© ×‘-registry.

    ×“×•×’×××•×ª:
    ---------
        from common.data_providers import TiingoFundamentalsProvider

        register_fundamentals_provider(TiingoFundamentalsProvider())

    ×•××–:
        load_index_fundamentals(..., provider="tiingo")
    """
    name = getattr(provider, "name", None) or provider.__class__.__name__.lower()
    name = str(name).lower().strip()
    if not name:
        raise ValueError("Provider must have a non-empty name")

    if name in _FUNDAMENTALS_PROVIDERS:
        logger.warning(
            "Overriding existing fundamentals provider %r with %r",
            name,
            provider,
        )
    _FUNDAMENTALS_PROVIDERS[name] = provider
    logger.info("Registered fundamentals provider: %s -> %r", name, provider)


def get_fundamentals_provider(name: str | None = None) -> BaseFundamentalsProvider:
    """
    ××§×‘×œ ××ª ×¡×¤×§ ×”×“××˜×” ×œ×¤×™ ×©× ×œ×•×’×™.

    ×× name=None â€“ ××©×ª××© ×‘-SETTINGS.FUNDAMENTALS_DEFAULT_PROVIDER.
    ×× ×œ× × ××¦× â€“ ×–×•×¨×§ KeyError ×¢× ×”×•×“×¢×” ×‘×¨×•×¨×”.
    """
    if name is None or not str(name).strip():
        name = SETTINGS.FUNDAMENTALS_DEFAULT_PROVIDER
    key = str(name).lower().strip()

    if key not in _FUNDAMENTALS_PROVIDERS:
        raise KeyError(
            f"No fundamentals provider registered under name {key!r}. "
            "Make sure to call register_fundamentals_provider(...) at startup."
        )
    return _FUNDAMENTALS_PROVIDERS[key]


# =========================
# Path helpers (symbols â†’ files)
# =========================

def _normalize_symbol(symbol: str) -> str:
    """
    ×× ×¨××œ ×¡×™××•×œ ×œ×©×™××•×© ×‘×‘×¡×™×¡ ×§×‘×¦×™×:
    - ××—×œ×™×£ '/' ×•-' ' ×‘-'_'
    - ××—×–×™×¨ ×‘××•×ª×™×•×ª ×’×“×•×œ×•×ª (SPY, QQQ...)
    """
    return str(symbol).strip().replace("/", "_").replace(" ", "_").upper()


def _symbol_base_path(symbol: str) -> Path:
    """
    ×‘×¡×™×¡ path ×¢×‘×•×¨ symbol â€“ ×œ×œ× ×¡×™×•××ª:
        data/fundamentals/SPY
    """
    sym = _normalize_symbol(symbol)
    base_dir = SETTINGS.FUNDAMENTALS_DATA_DIR
    return base_dir / sym


def _parquet_path(symbol: str) -> Path:
    """
    ×§×•×‘×¥ Parquet ×¢×‘×•×¨ symbol:
        data/fundamentals/SPY.parquet
    """
    return _symbol_base_path(symbol).with_suffix(".parquet")


def _csv_path(symbol: str) -> Path:
    """
    ×§×•×‘×¥ CSV ×¢×‘×•×¨ symbol:
        data/fundamentals/SPY.csv
    """
    return _symbol_base_path(symbol).with_suffix(".csv")


def _meta_path(symbol: str) -> Path:
    """
    ×§×•×‘×¥ meta.json ×¢×‘×•×¨ symbol:
        data/fundamentals/SPY.meta.json
    """
    return _symbol_base_path(symbol).with_suffix(".meta.json")


def _ensure_data_dir() -> None:
    """
    ××‘×˜×™×— ×©×”×ª×™×§×™×™×” FUNDAMENTALS_DATA_DIR ×§×™×™××ª.
    """
    SETTINGS.FUNDAMENTALS_DATA_DIR.mkdir(parents=True, exist_ok=True)


# ============================================================
# ×—×œ×§ 1/4 × ×’××¨ ×›××Ÿ.
# ×‘×—×œ×§ 2/4 × ×•×¡×™×£:
#   - ×§×¨×™××ª/×›×ª×™×‘×ª ×§×‘×¦×™× (parquet/csv)
#   - ×§×¨×™××”/×›×ª×™×‘×ª meta.json
#   - normalization ×œ-DataFrame (index = date, columns = snake_case)
#   - ××¢×˜×¤×ª ×‘×¡×™×¡×™×ª ×œ×©×™××•×© ×‘-BaseFundamentalsProvider
# ============================================================

# ============================================================
# Part 2/4 â€” Local I/O, Metadata, Normalization & Provider glue
# ============================================================
"""
×‘×—×œ×§ ×”×–×” ×× ×—× ×• ××•×¡×™×¤×™× ××ª ×”×™×›×•×œ×•×ª ×”×‘××•×ª:

1. ğŸ“‚ ×¢×‘×•×“×” ×¢× ×§×‘×¦×™× ×œ×•×§×œ×™×™× (Parquet / CSV):
   - _load_from_disk(symbol, require_fresh=True)
   - _save_to_disk(symbol, df, source, extra_meta)

2. ğŸ§¾ Metadata:
   - _is_fresh(path) â€“ ×‘×“×™×§×ª TTL ×œ×¤×™ SETTINGS.FUNDAMENTALS_CACHE_TTL_DAYS
   - _load_metadata(symbol) -> FundamentalMeta | None
   - _save_metadata(symbol, FundamentalMeta | dict)

3. ğŸ§¼ × ×™×¨××•×œ DataFrame ×œ×¤×•×¨××˜ ××—×™×“:
   - _normalize_fundamentals_df(df)
     * ××™× ×“×§×¡ ×–××Ÿ (DatetimeIndex) ×¢×œ ×‘×¡×™×¡ ×¢××•×“×•×ª date/period_end/report_date ×•×›×•'
     * ×©××•×ª ×¢××•×“×•×ª lower_snake_case
     * ××™×•×Ÿ ×œ×¤×™ ×ª××¨×™×š, ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª, ×”×¡×¨×ª ×›×œ-NaN ×‘×§×¦×•×•×ª

4. ğŸ”Œ ××¢×˜×¤×ª ×‘×¡×™×¡×™×ª ×œ×¢×‘×•×“×” ×¢× Provider:
   - _fetch_via_provider(symbol, start, end, fields, provider_name)
   - ×œ× × ×•×’×¢×™× ×¢×“×™×™×Ÿ ×‘-load_index_fundamentals (×–×” ×™×”×™×” ×‘×—×œ×§ 3).

×©×™××• ×œ×‘:
--------
- ××™×Ÿ ×›××Ÿ ×›× ×™×¡×” ×œ-API ×—×™×¦×•× ×™ â€“ ×–×” × ×¢×©×” ×“×¨×š BaseFundamentalsProvider.fetch.
- ×”×§×•×“ ×ª×•×× ×’× ×œ×¡×‘×™×‘×•×ª ×‘×”×Ÿ ×§×™×™××™× ×¨×§ CSV/Parquet (×œ×œ× ××™× ×˜×¨× ×˜).
"""

import json


# =========================
# Freshness / TTL helpers
# =========================

def _is_fresh(path: Path) -> bool:
    """
    ×‘×“×™×§×” ×× ×§×•×‘×¥ × ×—×©×‘ "×˜×¨×™" ×œ×¤×™ ×”×’×“×¨×ª ×”-TTL ×‘-SETTINGS.

    ×œ×•×’×™×§×”:
    -------
    - ×× FUNDAMENTALS_CACHE_TTL_DAYS <= 0 â†’ ×›×œ ×§×•×‘×¥ ×§×™×™× × ×—×©×‘ ×˜×¨×™.
    - ×× ×”×§×•×‘×¥ ×œ× ×§×™×™× â†’ False.
    - ××—×¨×ª, ××©×•×•×™× ××ª ×–××Ÿ ×”××•×“×™×¤×™×§×¦×™×” ×©×œ ×”×§×•×‘×¥ ×œ×¢×•××ª ×¢×›×©×™×•.
    """
    ttl_days = SETTINGS.FUNDAMENTALS_CACHE_TTL_DAYS
    if not path.exists():
        return False
    if ttl_days <= 0:
        return True

    try:
        mtime = datetime.fromtimestamp(path.stat().st_mtime)
    except OSError:  # pragma: no cover - edge filesystem
        return False

    age = datetime.now() - mtime
    return age <= timedelta(days=ttl_days)


# =========================
# Metadata helpers
# =========================

def _load_metadata(symbol: str) -> FundamentalMeta | None:
    """
    ×˜×•×¢×Ÿ FundamentalMeta ××¡×‘×™×‘×ª ×”×¢×‘×•×“×”, ×× ×§×™×™×.

    - ×× ××™×Ÿ meta.json â†’ ××—×–×™×¨ None.
    - ×× ×™×© ×‘×¢×™×” ×‘×§×¨×™××” â†’ ×›×•×ª×‘ ××–×”×¨×” ×•××—×–×™×¨ None.
    """
    meta_p = _meta_path(symbol)
    if not meta_p.exists():
        return None

    try:
        with meta_p.open("r", encoding="utf-8") as f:
            raw = json.load(f)
        meta = FundamentalMeta.from_dict(raw)
        return meta
    except Exception as exc:  # pragma: no cover - ×œ×•×’ ×©×§×˜ ×‘×™×™×¦×•×¨
        logger.warning(
            "Failed to read fundamentals meta for %s from %s: %s",
            symbol,
            meta_p,
            exc,
        )
        return None


def _save_metadata(
    symbol: str,
    meta: FundamentalMeta | Mapping[str, Any],
) -> None:
    """
    ×©×•××¨ FundamentalMeta (××• dict ×“×•××”) ×œ×§×•×‘×¥ meta.json.

    ×©×™××•×© ×¤× ×™××™ ××ª×•×š _save_to_disk.
    """
    if isinstance(meta, FundamentalMeta):
        data = meta.to_dict()
    else:
        data = dict(meta)

    safe = make_json_safe(data)
    meta_p = _meta_path(symbol)

    try:
        with meta_p.open("w", encoding="utf-8") as f:
            json.dump(
                safe,
                f,
                ensure_ascii=False,
                indent=2,
                default=_json_default,
            )
        logger.debug("Saved fundamentals meta for %s to %s", symbol, meta_p)
    except Exception as exc:  # pragma: no cover
        logger.warning(
            "Failed to write fundamentals meta for %s to %s: %s",
            symbol,
            meta_p,
            exc,
        )


# =========================
# DataFrame normalization
# =========================

_DATE_COLUMN_CANDIDATES = (
    "date",
    "datetime",
    "period_end",
    "report_date",
    "as_of_date",
    "statement_date",
)


def _guess_date_column(df: pd.DataFrame) -> str | None:
    """
    ××–×”×” ×¢××•×“×ª ×ª××¨×™×š ×¡×‘×™×¨×” ××ª×•×š DataFrame ×œ×¤×™ ×¨×©×™××ª ×©××•×ª ×˜×™×¤×•×¡×™×™×.

    ×× ×œ× ××•×¦× â€“ ××—×–×™×¨ None.
    """
    cols_lower = {c.lower(): c for c in df.columns}
    for cand in _DATE_COLUMN_CANDIDATES:
        if cand in cols_lower:
            return cols_lower[cand]
    return None


def _normalize_column_name(name: Any) -> str:
    """
    ×× ×¨××œ ×©× ×¢××•×“×” ×œ×¤×•×¨××˜ lower_snake_case ×¤×©×•×˜.
    ×“×•×’×××•×ª:
        'P/E' -> 'p_e'
        'Price to Book' -> 'price_to_book'
        'ROE (%)' -> 'roe'
    """
    s = str(name).strip()

    # ×”×¡×¨×” ×©×œ ×¡×™××•×œ×™× × ×¤×•×¦×™×
    remove_chars = ["%", "(", ")", "[", "]"]
    for ch in remove_chars:
        s = s.replace(ch, "")

    # ×”×¤×¨×“×” ×‘×¡×™×¡×™×ª
    s = (
        s.replace("/", "_")
        .replace("-", "_")
        .replace(" ", "_")
        .replace("__", "_")
    )

    s = s.lower()
    return s


def _normalize_fundamentals_df(df: pd.DataFrame) -> FundamentalFrame:
    """
    ×× ×¨××œ DataFrame ×©××’×™×¢ ××¡×¤×§/×§×•×‘×¥ ×œ×¤×•×¨××˜ ××—×™×“:

    1. ××‘×˜×™×— index ××‘×•×¡×¡ ×–××Ÿ (DatetimeIndex).
       - ×× ××™×Ÿ ××™× ×“×§×¡ ×›×–×”, ××—×¤×© ×¢××•×“×ª ×ª××¨×™×š ×˜×™×¤×•×¡×™×ª (date/period_end/...).
       - ×× ××•×¦× â€“ ×”×•×¤×š ××•×ª×” ×œ-DatetimeIndex.
       - ×× ×œ× â€“ ××©××™×¨ ××™× ×“×§×¡ ×›×¤×™ ×©×”×•× ××‘×œ ××ª×¨×™×¢ ×‘×œ×•×’.

    2. ×× ×¨××œ ×©××•×ª ×¢××•×“×•×ª ×œ-lower_snake_case.

    3. ×××™×™×Ÿ ×œ×¤×™ ×–××Ÿ, ××¡×™×¨ ×›×¤×™×œ×•×™×•×ª ×¢×œ index, ×•××¡×™×¨ ×©×•×¨×•×ª ×¨×™×§×•×ª ×œ×’××¨×™.

    ×©×™× ×œ×‘:
    -------
    - ×œ× ××‘×¦×¢ resample ×œ×ª×“×™×¨×•×ª ×§×‘×•×¢×” (×–×” ×™×”×™×” ×ª×¤×§×™×“ build_fundamentals_panel).
    - ×œ× ×›×•×¤×” ×˜×™×¤×•×¡×™× ××¡×¤×¨×™×™× â€“ ××‘×œ × ×™×ª×Ÿ ×œ×”×•×¡×™×£ ×‘×”××©×š coercion ×œ×¤×™ ×¦×•×¨×š.
    """
    if df is None or df.empty:
        # ××—×–×™×¨ DataFrame ×¨×™×§ ××‘×œ ×©×•××¨ ×¢×œ ×˜×™×¤×•×¡
        return pd.DataFrame()

    df = df.copy()

    # --- ×˜×™×¤×•×œ ×‘××™× ×“×§×¡ ×–××Ÿ ---
    if not isinstance(df.index, pd.DatetimeIndex):
        # × × ×¡×” ×œ×–×”×•×ª ×¢××•×“×ª ×ª××¨×™×š
        date_col = _guess_date_column(df)
        if date_col is not None:
            try:
                df[date_col] = pd.to_datetime(df[date_col])
                df = df.set_index(date_col)
            except Exception as exc:  # pragma: no cover
                logger.warning(
                    "Failed to convert column %r to datetime index in fundamentals DF: %s",
                    date_col,
                    exc,
                )
        else:
            # ××™×Ÿ ×¢××•×“×ª ×ª××¨×™×š â€“ × ×ª×¨×™×¢, ××‘×œ ×œ× × ×–×¨×•×§ ×©×’×™××”
            logger.warning(
                "Fundamentals DataFrame has no obvious date column; "
                "consider including 'date' or 'period_end'."
            )

    # ×× ×™×© ××™× ×“×§×¡ ×–××Ÿ â€“ × ××™×™×Ÿ
    if isinstance(df.index, pd.DatetimeIndex):
        df = df.sort_index()
        # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×‘×ª××¨×™×š â€“ × ×©××•×¨ ××ª ×”××—×¨×•× ×” (××• ××¤×©×¨ ×œ×‘×—×•×¨ ××—×¨×ª)
        df = df[~df.index.duplicated(keep="last")]

    # --- × ×™×¨××•×œ ×©××•×ª ×¢××•×“×•×ª ---
    df.columns = [_normalize_column_name(c) for c in df.columns]

    # --- ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª ×œ×—×œ×•×˜×™×Ÿ ---
    # (×›×œ ×”×¢×¨×›×™× NaN ××• None)
    df = df.dropna(how="all")

    return df


# =========================
# Local I/O (Parquet / CSV)
# =========================

def _load_from_parquet(path: Path) -> pd.DataFrame | None:
    """
    × ×™×¡×™×•×Ÿ ×œ×˜×¢×•×Ÿ DataFrame ××§×•×‘×¥ Parquet.

    ×× ×™×© ×›×©×œ â€“ ××—×–×™×¨ None ×•×œ× ×–×•×¨×§ ×”×—×•×¦×” (××©××™×¨ ×œ×œ×•×’ ×œ×˜×¤×œ).
    """
    if not path.exists():
        return None
    try:
        df = pd.read_parquet(path)
        logger.debug("Loaded fundamentals parquet from %s", path)
        return df
    except Exception as exc:  # pragma: no cover
        logger.warning("Failed to read parquet fundamentals %s: %s", path, exc)
        return None


def _load_from_csv(path: Path) -> pd.DataFrame | None:
    """
    × ×™×¡×™×•×Ÿ ×œ×˜×¢×•×Ÿ DataFrame ××§×•×‘×¥ CSV.

    ×× ×™×© ×›×©×œ â€“ ××—×–×™×¨ None ×•×œ× ×–×•×¨×§ ×”×—×•×¦×” (××©××™×¨ ×œ×œ×•×’ ×œ×˜×¤×œ).
    """
    if not path.exists():
        return None
    try:
        df = pd.read_csv(path)
        logger.debug("Loaded fundamentals csv from %s", path)
        return df
    except Exception as exc:  # pragma: no cover
        logger.warning("Failed to read csv fundamentals %s: %s", path, exc)
        return None


def _load_from_disk(
    symbol: str,
    *,
    require_fresh: bool = True,
) -> FundamentalFrame | None:
    """
    ×˜×•×¢×Ÿ ×¤× ×“×•×× ×˜×œ ×œ-symbol ××”×“×™×¡×§ (Parquet/CSV), ×× ×§×™×™×.

    ×¤×¨××˜×¨×™×
    --------
    symbol : str
        ×¡×™××•×œ ×”××“×“/ETF (SPY, QQQ ×•×›×•').
    require_fresh : bool
        ×× True â€“ × ×‘×“×•×§ TTL ×¢× _is_fresh, ×•× ×—×–×™×¨ None ×× ×”×§×•×‘×¥ ×™×©×Ÿ ××“×™.
        ×× False â€“ × ×˜×¢×Ÿ ×’× ×§×•×‘×¥ ×™×©×Ÿ.

    ×”×—×–×¨
    -----
    DataFrame ××• None:
        - DataFrame ×× ×•×¨××œ ×‘×××¦×¢×•×ª _normalize_fundamentals_df ×× × ××¦× ×§×•×‘×¥.
        - None ×× ××™×Ÿ ×§×•×‘×¥ / ×œ× ×˜×¨×™ / ×›×©×œ ×§×¨×™××”.
    """
    _ensure_data_dir()
    sym_norm = _normalize_symbol(symbol)
    pq = _parquet_path(sym_norm)
    csv = _csv_path(sym_norm)

    if require_fresh:
        # ×× ×œ× ×˜×¨×™ â€“ ×œ× × × ×¡×” ×‘×›×œ×œ ×œ×˜×¢×•×Ÿ (× ×¢×“×™×£ provider)
        if not (_is_fresh(pq) or _is_fresh(csv)):
            logger.debug(
                "No fresh fundamentals cache for %s (ttl=%sd) â€“ skipping local load",
                sym_norm,
                SETTINGS.FUNDAMENTALS_CACHE_TTL_DAYS,
            )
            return None

    df_raw: pd.DataFrame | None = None

    # ×¢×“×™×£ Parquet ×¢×œ CSV
    df_raw = _load_from_parquet(pq)
    if df_raw is None:
        df_raw = _load_from_csv(csv)

    if df_raw is None:
        return None

    df_norm = _normalize_fundamentals_df(df_raw)
    return df_norm


def _save_to_disk(
    symbol: str,
    df: FundamentalFrame,
    *,
    source: str,
    extra_meta: Mapping[str, Any] | None = None,
) -> None:
    """
    ×©×•××¨ DataFrame ×¤× ×“×•×× ×˜×œ ×œ×§×‘×¦×™ Parquet/CSV + ×§×•×‘×¥ ××˜×-×“××˜×”.

    ×¤×¨××˜×¨×™×
    --------
    symbol : str
        ×¡×™××•×œ ×”××“×“/ETF (SPY, QQQ ×•×›×•').
    df : DataFrame
        ×“××˜×” ×× ×•×¨××œ (×¢×“×™×£ ×©×›×‘×¨ ×¢×‘×¨ _normalize_fundamentals_df).
    source : str
        ×ª×™××•×¨ ××§×•×¨ ×”×“××˜×” (×œ××©×œ "tiingo", "ibkr", "local_file").
    extra_meta : Mapping[str, Any] | None
        ×©×“×•×ª ××˜× × ×•×¡×¤×™× ×œ×©××™×¨×” (×œ××©×œ ×›××•×ª nulls, ×’×¨×¡×ª schema ×•×›×•').
    """
    _ensure_data_dir()
    sym_norm = _normalize_symbol(symbol)
    pq = _parquet_path(sym_norm)
    csv = _csv_path(sym_norm)

    # ×§×•×“× × ×©××•×¨ ××ª ×”×“××˜×” ×¢×¦××•
    try:
        df.to_parquet(pq)
        logger.info("Saved fundamentals for %s to %s", sym_norm, pq)
    except Exception as exc:  # pragma: no cover
        logger.warning("Failed to write parquet fundamentals %s: %s", pq, exc)

    try:
        df.to_csv(csv, index=True)
        logger.debug("Also saved fundamentals for %s to %s", sym_norm, csv)
    except Exception as exc:  # pragma: no cover
        logger.warning("Failed to write csv fundamentals %s: %s", csv, exc)

    # ×›×¢×ª × ×‘× ×” ×•× ×©××•×¨ meta
    meta = FundamentalMeta(
        symbol=sym_norm,
        source=source,
        last_refresh=datetime.now(),
        n_rows=int(df.shape[0]),
        n_cols=int(df.shape[1]),
        fields=list(df.columns),
    )
    if extra_meta:
        # × ×›×œ×•×œ ××™×“×¢ × ×•×¡×£ ×‘×ª×•×š dict ×•× ×©××™×¨ ×œ-FundamentalMeta ×¨×§ ××ª ×”×‘×¡×™×¡
        # × ×™×ª×Ÿ ×œ×”×¨×—×™×‘ ×‘×¢×ª×™×“ ×œ×¡×›×™××” ×¢×©×™×¨×” ×™×•×ª×¨
        merged = {**meta.to_dict(), **dict(extra_meta)}
        _save_metadata(sym_norm, merged)
    else:
        _save_metadata(sym_norm, meta)


# =========================
# Provider glue
# =========================

def _fetch_via_provider(
    symbol: str,
    *,
    provider_name: str | None = None,
    start: date | None = None,
    end: date | None = None,
    fields: Sequence[str] | None = None,
) -> tuple[FundamentalFrame, str]:
    """
    Fetch fundamentals ×¢×‘×•×¨ symbol ×“×¨×š Provider ×”×¨×©×•× ×‘-registry.

    ×”×—×•×–×”:
    -------
    - ××—×–×™×¨×™× tuple:
        (df_norm, provider_name_effective)

    - df_norm:
        DataFrame **×× ×•×¨××œ** (_normalize_fundamentals_df) ×¢×:
        index: DatetimeIndex (×× ××¤×©×¨)
        columns: lower_snake_case.

    - provider_name_effective:
        ×”×©× ×©×”×©×ª××©× ×• ×‘×• ×‘×¤×•×¢×œ (×œ××©×œ "tiingo", ×’× ×× provider_name=None).

    ×”×ª× ×”×’×•×ª:
    --------
    - ×× ××™×Ÿ ×¡×¤×§ ×‘×¨×™×©×•× â†’ KeyError.
    - ×× ×”×¡×¤×§ ×–×•×¨×§ ×©×’×™××” â†’ × ×¢×‘×™×¨ ××•×ª×” (×©×™××•×©×™ ×›×“×™ ×œ×“×¢×ª ××” ×§×¨×”).
    """
    if provider_name is None or not str(provider_name).strip():
        provider_name = SETTINGS.FUNDAMENTALS_DEFAULT_PROVIDER
    provider_name = str(provider_name).lower().strip()

    provider = get_fundamentals_provider(provider_name)

    logger.info(
        "Fetching fundamentals for %s via provider %s (start=%s, end=%s, fields=%s)",
        symbol,
        provider_name,
        start,
        end,
        fields,
    )

    df_raw = provider.fetch(
        symbol,
        start=start,
        end=end,
        fields=fields,
    )

    df_norm = _normalize_fundamentals_df(df_raw)
    return df_norm, provider_name


# =========================
# Diagnostics helpers (optional)
# =========================

def list_cached_symbols() -> list[str]:
    """
    ××—×–×™×¨ ×¨×©×™××ª ×¡×™××•×œ×™× ×©×™×© ×œ×”× ×§×‘×¦×™ fundamentals ×‘×ª×™×§×™×™×ª ×”×“××˜×”.

    ×©×™××•×©×™ ×œ-debug / ×œ×•×—×•×ª ×‘×§×¨×”:
    - ×œ×¨××•×ª ×‘××” ×”××¢×¨×›×ª ×›×‘×¨ ××›×•×¡×”.
    - ×œ×¢×§×•×‘ ××—×¨×™ ×›××•×ª ×”×¡×™××•×œ×™× ×”×× ×•×˜×¨×™×.
    """
    base = SETTINGS.FUNDAMENTALS_DATA_DIR
    if not base.exists():
        return []

    symbols: set[str] = set()
    for p in base.iterdir():
        if not p.is_file():
            continue
        if p.suffix.lower() not in (".parquet", ".csv"):
            continue
        stem = p.stem  # ×œ××©×œ "SPY"
        symbols.add(stem.upper())

    return sorted(symbols)


def get_cached_meta(symbol: str) -> FundamentalMeta | None:
    """
    Wrapper ×™×“×™×“×•×ª×™ ×œ-_load_metadata: ××—×–×™×¨ FundamentalMeta ×× ×™×©.

    ×–×” ×™×›×•×œ ×œ×©××©:
    - ×œ×”×¦×’×ª "Last refresh" ×‘-UI.
    - × ×™×˜×•×¨ ×¡××•×™ (×œ×•×’×™×/health check).
    """
    sym_norm = _normalize_symbol(symbol)
    return _load_metadata(sym_norm)


# ×¡×•×£ ×—×œ×§ 2/4.
# ×‘×—×œ×§ 3/4 × ×•×¡×™×£ ××ª ×”-API ×”×¦×™×‘×•×¨×™:
#   load_index_fundamentals / load_index_fundamentals_cached /
#   load_indices_fundamentals / build_fundamentals_panel
# ×”××©×ª××©×™× ×‘×™×›×•×œ×•×ª ×©×”×•×’×“×¨×• ×›××Ÿ.
# ============================================================
# Part 3/4 â€” Public API: load / cache / panel construction
# ============================================================
"""
×‘×—×œ×§ ×”×–×” ×× ×—× ×• ×‘×•× ×™× ××ª ×”-API ×”×¦×™×‘×•×¨×™ ×©×œ ×©×›×‘×ª ×”×¤× ×“×•×× ×˜×œ ×œ××“×“×™×:

1. ğŸ” load_index_fundamentals(...)
   - ×˜×•×¢×Ÿ DataFrame ×¤× ×“×•×× ×˜×œ ×œ××“×“/ETF ×‘×•×“×“.
   - ××©×œ×‘ Cache ×œ×•×§×œ×™ + ×¡×¤×§ ×—×™×¦×•× ×™ (provider).
   - ×©×•×œ×˜ ×‘-TTL, force_refresh, allow_remote, allow_partial.

2. ğŸ§  load_index_fundamentals_cached(...)
   - ×¢×˜×™×¤×” ×¢× LRU cache ×œ×©×™××•×© ×‘×ª×•×š ×¨×™×¦×” ××—×ª (session).
   - ××™×•×¢×“×ª ×œ×”×™×× ×¢ ××˜×¢×™× ×•×ª ×—×•×–×¨×•×ª ×œ××•×ª×• ×¡×™××•×œ/×˜×•×•×—/fields.

3. ğŸ“š load_indices_fundamentals(...)
   - ×˜×¢×™× ×” ×œ-universe ×©×œ×: {symbol -> DataFrame}.

4. ğŸ§± build_fundamentals_panel(...)
   - ×‘×•× ×” Panel ×‘×¤×•×¨××˜ MultiIndex (date, symbol) ×œ×›×œ ×”-universe.
   - ××•×¤×¦×™×•× ×œ×™×ª: resample ×œ×ª×“×™×¨×•×ª ××—×™×“×” (D/W/M/Q).
   - ×ª×•××š ×‘-forward-fill (×œ××©×œ ×œ×”×¤×•×š ×¨×‘×¢×•×Ÿ/×©× ×” ×œ× ×ª×•×Ÿ ×—×•×“×©×™).

×”×›×œ × ×‘× ×” ×›×š ×©×™×ª××™× ×œ××” ×©×ª×›× × ×•:
- 10 ×©× ×™× ×©×œ ×“××˜×”.
- ×›×™×¡×•×™ ×¨×—×‘ ×©×œ ×¤×™×¦'×¨×™×.
- ×ª××™×›×” ×‘×”×¨×—×‘×” ×œ-Regimes / Clustering / Signals ×‘×¨××ª core.
"""


# =========================
# Internal helpers (fields & ranges)
# =========================

def _select_fields(
    df: FundamentalFrame,
    fields: Sequence[str] | None,
    *,
    allow_partial: bool,
    symbol: str,
) -> FundamentalFrame:
    """
    ×‘×—×™×¨×ª ×¢××•×“×•×ª ×œ×¤×™ ×¨×©×™××ª fields, ×¢× ×ª××™×›×” ×‘-allow_partial.

    - fields=None â†’ ××—×–×™×¨ ××ª df ×›××• ×©×”×•×.
    - ××—×¨×ª:
        * ×× ×¨××œ ××ª ×”×©××•×ª ×œ×‘×¡×™×¡ lower-case ×œ×”×©×•×•××”.
        * ×× allow_partial=True â†’ ××—×–×™×¨ ×¨×§ ××” ×©×§×™×™×, ××ª×¨×™×¢ ×¢×œ ×—×¡×¨×™×.
        * ×× allow_partial=False â†’ ×× ×—×¡×¨×•×ª ×¢××•×“×•×ª, ××¢×œ×” ValueError.
    """
    if df.empty:
        return df

    if not fields:
        return df

    # normalization ×œ-lower ×¢×‘×•×¨ mapping
    cols_lower = {c.lower(): c for c in df.columns}
    requested_norm = [str(f).lower().strip() for f in fields]

    found_cols: list[str] = []
    missing: list[str] = []
    for f in requested_norm:
        if f in cols_lower:
            found_cols.append(cols_lower[f])
        else:
            missing.append(f)

    if missing:
        msg = (
            f"Fundamental fields {missing!r} not found for symbol {symbol!r}. "
            f"Available fields: {list(df.columns)!r}"
        )
        if allow_partial:
            logger.warning(msg + " (allow_partial=True â†’ continuing with subset)")
        else:
            raise ValueError(msg + " (allow_partial=False â†’ aborting)")

    if found_cols:
        return df[found_cols].copy()
    else:
        # ××™×Ÿ ××¤×™×œ×• ×¢××•×“×” ××—×ª ××”××‘×•×§×©×•×ª
        if allow_partial:
            # × ×—×–×™×¨ DataFrame ×¨×™×§ ×¢× ××•×ª× ××™× ×“×§×¡×™× (××•×œ×™ ×¢×“×™×™×Ÿ × ×¨×¦×” ×¨×§ Timeline)
            logger.warning(
                "No requested fundamental fields were found for symbol %r; "
                "returning empty DataFrame with same index.",
                symbol,
            )
            return pd.DataFrame(index=df.index)
        else:
            raise ValueError(
                f"No requested fundamental fields found for {symbol!r} and "
                "allow_partial=False."
            )


def _apply_date_range(
    df: FundamentalFrame,
    start: date | None,
    end: date | None,
) -> FundamentalFrame:
    """
    ×—×•×ª×š DataFrame ×œ×¤×™ ×˜×•×•×— ×ª××¨×™×›×™× (×× ×™×© ××™× ×“×§×¡ ×–××Ÿ).
    """
    if df.empty:
        return df

    if not isinstance(df.index, pd.DatetimeIndex):
        return df

    df_out = df
    if start is not None:
        df_out = df_out[df_out.index >= pd.to_datetime(start)]
    if end is not None:
        df_out = df_out[df_out.index <= pd.to_datetime(end)]
    return df_out


def _coerce_allow_partial(allow_partial: bool | None) -> bool:
    """
    ×× ×œ× ×¡×•×¤×§ allow_partial, × ×©×ª××© ×‘×‘×¨×™×¨×ª ××—×“×œ ××”-SETTINGS.
    """
    if allow_partial is None:
        return SETTINGS.FUNDAMENTALS_ALLOW_PARTIAL
    return bool(allow_partial)


def _coerce_require_fresh_local(require_fresh_local: bool | None) -> bool:
    """
    ×‘×¨×™×¨×ª ××—×“×œ: if require_fresh_local=None â†’ True
    (×›×œ×•××¨, ×‘××•×¤×Ÿ ×“×™×¤×•×œ×˜×™ ×œ× × ×¡×ª××š ×¢×œ ×§×•×‘×¥ ×™×©×Ÿ ×× TTL ×¤×’).
    """
    if require_fresh_local is None:
        return True
    return bool(require_fresh_local)


def _coerce_force_refresh(force_refresh: bool | None) -> bool:
    """
    ×‘×¨×™×¨×ª ××—×“×œ: ×œ×•×§×— ××”-SETTINGS.FUNDAMENTALS_ALWAYS_REFRESH.
    """
    if force_refresh is None:
        return SETTINGS.FUNDAMENTALS_ALWAYS_REFRESH
    return bool(force_refresh)


# =========================
# Public API â€” single symbol
# =========================

def load_index_fundamentals(
    symbol: str,
    *,
    start: date | None = None,
    end: date | None = None,
    fields: Sequence[str] | None = None,
    provider: str | None = None,
    allow_remote: bool = True,
    force_refresh: bool | None = None,
    require_fresh_local: bool | None = None,
    allow_partial: bool | None = None,
) -> FundamentalFrame:
    """
    ×˜×•×¢×Ÿ ××ª ×›×œ ×”×™×¡×˜×•×¨×™×™×ª ×”×“××˜×” ×”×¤× ×“×•×× ×˜×œ×™ ×œ××“×“/ETF ×‘×•×“×“, ×ª×•×š × ×™×”×•×œ cache ×‘×¨××ª
    ×§×¨×Ÿ ×’×™×“×•×¨:

    ×œ×•×’×™×§×” high-level:
    -------------------
    1. × ×™×¡×™×•×Ÿ ×˜×¢×™× ×” ××”×“×™×¡×§:
       - ×× require_fresh_local=True â†’ × ×‘×“×•×§ TTL (×˜×¨×™×•×ª) ×¢× _is_fresh.
       - ×× force_refresh=True â†’ × ×“×œ×’ ×¢×œ ×”×§×•×‘×¥ ×•× ×œ×š ×™×©×¨ ×œ-provider.

    2. ×× ×™×© df ×œ×•×§×œ×™:
       - × × ×¨××œ (×× ×œ× ×›×‘×¨).
       - × ×—×ª×•×š ×œ×¤×™ ×˜×•×•×— ×ª××¨×™×›×™× (start/end).
       - × ×‘×—×¨ ×¢××•×“×•×ª (fields) ×œ×¤×™ allow_partial.
       - ×× ×–×” ××¡×¤×™×§ ×œ× ×• â†’ × ×—×–×™×¨.

       "××¡×¤×™×§ ×œ× ×•" = ××• ×©××™×Ÿ fields ×¡×¤×¦×™×¤×™×™×,
                      ××• ×©×›×œ ×”-fields ×§×™×™××™×,
                      ××• allow_partial=True (××•×ª×¨ subset).

    3. ×× ××™×Ÿ df ×œ×•×§×œ×™ ××ª××™× / force_refresh=True / ×—×¡×¨×™× ×©×“×•×ª ×§×¨×™×˜×™×™×:
       - ×× allow_remote=False â†’ × ×–×¨×•×§ FileNotFoundError/ValueError ×¨×œ×•×•× ×˜×™.
       - ××—×¨×ª:
           * × ×©×ª××© ×‘-provider ×“×¨×š _fetch_via_provider.
           * × × ×¨××œ, × ×—×ª×•×š ×˜×•×•×— ×ª××¨×™×›×™×, × ×‘×—×¨ fields.
           * × ×©××•×¨ ×œ×“×™×¡×§ (Parquet/CSV + meta).
           * × ×—×–×™×¨.

    ×¤×¨××˜×¨×™×
    --------
    symbol : str
        ×¡×™××•×œ ×”××“×“/ETF (SPY, QQQ, IWM, EEM ×•×›×•').
    start, end : date | None
        ×˜×•×•×— ×ª××¨×™×›×™× ×œ×¤×™×œ×˜×•×¨ ×”××™× ×“×§×¡.
    fields : Sequence[str] | None
        ×¨×©×™××ª ×©×“×•×ª ×¤× ×“×•×× ×˜×œ×™×™× (pe, pb, roe, dividend_yield ×•×›×•').
        ×× None â€“ ××—×–×™×¨ ×›×œ ×”×¢××•×“×•×ª.
    provider : str | None
        ×©× ×¡×¤×§ (×›×¤×™ ×©× ×¨×©× ×‘-register_fundamentals_provider).
        ×× None â€“ ×™×©×ª××© ×‘-SETTINGS.FUNDAMENTALS_DEFAULT_PROVIDER.
    allow_remote : bool
        ×× False â€“ ××•× ×¢ ×¤× ×™×” ×œ×¡×¤×§, ×™×¡×ª××š ×¨×§ ×¢×œ ×§×‘×¦×™× ×œ×•×§×œ×™×™× ××• ×™×–×¨×•×§ ×©×’×™××”.
    force_refresh : bool | None
        ×× True â€“ ×™×ª×¢×œ× ×-cache ×•×™×¤× ×” ×œ×¡×¤×§ (×× allow_remote=True).
        ×× None â€“ ××©×ª××© ×‘-SETTINGS.FUNDAMENTALS_ALWAYS_REFRESH.
    require_fresh_local : bool | None
        ×× True â€“ ×œ× ×™×˜×¢×Ÿ ×§×•×‘×¥ ×™×©×Ÿ (TTL ×¢×‘×¨).
        ×× False â€“ ×™×¡×›×™× ×œ×˜×¢×•×Ÿ ×’× ×§×•×‘×¥ ×™×©×Ÿ.
        ×× None â€“ ×‘×¨×™×¨×ª ××—×“×œ True.
    allow_partial : bool | None
        ×× True â€“ ×× ×—×¡×¨×™× ×—×œ×§ ××”×©×“×•×ª, × ××©×™×š ×¢× ××” ×©×™×© (×•× ×¨×©×•× ××–×”×¨×”).
        ×× False â€“ ×× ×—×¡×¨×™× ×©×“×•×ª, × ×–×¨×•×§ ValueError.
        ×× None â€“ ××©×ª××© ×‘-SETTINGS.FUNDAMENTALS_ALLOW_PARTIAL.

    ×”×—×–×¨
    -----
    DataFrame:
        ××™× ×“×§×¡ = DatetimeIndex (×× ××¤×©×¨),
        ×¢××•×“×•×ª = ×©×“×•×ª ×¤× ×“×•×× ×˜×œ×™×™× ×× ×•×¨××œ×™× (lower_snake_case).
    """
    if not symbol or not str(symbol).strip():
        raise ValueError("symbol must be a non-empty string")

    sym_norm = _normalize_symbol(symbol)
    use_force_refresh = _coerce_force_refresh(force_refresh)
    use_require_fresh_local = _coerce_require_fresh_local(require_fresh_local)
    use_allow_partial = _coerce_allow_partial(allow_partial)

    logger.debug(
        "load_index_fundamentals(symbol=%s, start=%s, end=%s, fields=%s, "
        "provider=%s, allow_remote=%s, force_refresh=%s, require_fresh_local=%s, "
        "allow_partial=%s)",
        sym_norm,
        start,
        end,
        fields,
        provider,
        allow_remote,
        use_force_refresh,
        use_require_fresh_local,
        use_allow_partial,
    )

    # ---------------------------------------------------------
    # 1) × ×¡×™×•×Ÿ ×˜×¢×™× ×” ×œ×•×§××œ×™×ª (×× ×œ× ×‘×™×§×©×• force_refresh)
    # ---------------------------------------------------------
    df_local: FundamentalFrame | None = None
    if not use_force_refresh:
        df_local = _load_from_disk(
            sym_norm,
            require_fresh=use_require_fresh_local,
        )

    if df_local is not None and not df_local.empty:
        # × ×—×ª×•×š ×˜×•×•×— + ×©×“×•×ª
        df_local = _apply_date_range(df_local, start, end)
        df_local = _select_fields(
            df_local,
            fields,
            allow_partial=use_allow_partial,
            symbol=sym_norm,
        )

        # ×× ×‘×™×§×©×• partial ×•××¦×× ×• ×œ×¤×—×•×ª ×—×œ×§ â†’ ××¤×©×¨ ×œ×”×—×–×™×¨
        if fields is None or df_local.shape[1] > 0:
            logger.info(
                "Returning fundamentals for %s from local cache (rows=%s, cols=%s)",
                sym_norm,
                df_local.shape[0],
                df_local.shape[1],
            )
            return df_local

    # ---------------------------------------------------------
    # 2) ×× ××™×Ÿ ××• ×œ× ××¡×¤×™×§ â†’ ×¡×¤×§ ××¨×•×—×§ (×× ××•×ª×¨)
    # ---------------------------------------------------------
    if not allow_remote:
        # ××™×Ÿ remote, ×•××™×Ÿ df_local ×˜×•×‘ â†’ ×©×’×™××”
        if df_local is None:
            raise FileNotFoundError(
                f"No suitable fundamentals cache for {sym_norm} and allow_remote=False"
            )
        # df_local ×§×™×™× ××‘×œ ×—×¡×¨ fields ×§×¨×™×˜×™×™× (allow_partial=False)
        raise ValueError(
            f"Local fundamentals cache for {sym_norm} is insufficient and "
            f"allow_remote=False."
        )

    # ×¤× ×™×” ×œ×¡×¤×§ (×©× ×¡×¤×§ ××¤×§×˜×™×‘×™)
    df_remote, effective_provider = _fetch_via_provider(
        sym_norm,
        provider_name=provider,
        start=start,
        end=end,
        fields=fields,  # ××•×ª×¨ ×œ×¡×¤×§ ×œ× ×¡×•×ª ×œ×¤×œ×˜×¨ ×‘×¢×¦××•
    )

    # × × ×¨××œ / × ×—×ª×•×š ×©×•×‘ (×œ×¤×—×•×ª ×›×“×™ ×œ×”×‘×˜×™×— ×¢×§×‘×™×•×ª ××œ××”)
    df_remote = _normalize_fundamentals_df(df_remote)
    df_remote = _apply_date_range(df_remote, start, end)
    df_remote = _select_fields(
        df_remote,
        fields,
        allow_partial=use_allow_partial,
        symbol=sym_norm,
    )

    # ×©××™×¨×” ×œ×§×‘×¦×™×
    _save_to_disk(
        sym_norm,
        df_remote,
        source=effective_provider,
        extra_meta={
            "start": start.isoformat() if start else None,
            "end": end.isoformat() if end else None,
            "requested_fields": list(fields) if fields else None,
        },
    )

    logger.info(
        "Returning fundamentals for %s from provider %s (rows=%s, cols=%s)",
        sym_norm,
        effective_provider,
        df_remote.shape[0],
        df_remote.shape[1],
    )

    return df_remote


# =========================
# Cached single-symbol API
# =========================

@lru_cache(maxsize=256)
def load_index_fundamentals_cached(
    symbol: str,
    start: date | None = None,
    end: date | None = None,
    fields: tuple[str, ...] | None = None,
) -> FundamentalFrame:
    """
    ×¢×˜×™×¤×” ×¢× LRU cache ×¢×‘×•×¨ load_index_fundamentals, ×œ×©×™××•×© ×‘×ª×•×š Session ××—×“.

    ×”×¢×¨×•×ª:
    ------
    - fields ××§×‘×œ tuple ×›×“×™ ×©×™×”×™×” hashable ×¢×‘×•×¨ ×”-cache.
    - ×œ× ×›×•×œ×œ ×¤×¨××˜×¨×™× ×›××• provider/allow_remote/force_refresh â€“ ×–×”
      ××™×•×¢×“ ×‘×¢×™×§×¨ ×œ×©×™××•×© ×‘×ª×•×š core ×›×©×× ×—× ×• ×›×‘×¨ ×‘-trusted environment.
    - ×× ×ª×¨×¦×” cache-aware ×’× ×œ×¤×¨××˜×¨×™× ××ª×§×“××™×, ××¤×©×¨ ×œ×”×¨×—×™×‘ ×œ×’×¨×¡×”
      × ×•×¡×¤×ª (××• ×œ×”×¢×‘×™×¨ dict frozen).
    """
    # ×›××Ÿ × ×‘×—×¨ ×”×ª× ×”×’×•×ª ×©××¨× ×™×ª:
    # - allow_remote=True
    # - force_refresh=False
    # - require_fresh_local=True
    # - allow_partial=True (×›×™ ××™ ×©×¤×•× ×” ×œ×›××Ÿ ×›× ×¨××” ×‘×¡×“×¨ ×¢× subset/×‘×¨×™×¨×ª ××—×“×œ)
    fields_list: Sequence[str] | None = list(fields) if fields is not None else None
    return load_index_fundamentals(
        symbol,
        start=start,
        end=end,
        fields=fields_list,
        allow_remote=True,
        force_refresh=False,
        require_fresh_local=True,
        allow_partial=True,
    )


# =========================
# Public API â€” multi-symbol
# =========================

def load_indices_fundamentals(
    symbols: Sequence[str],
    *,
    start: date | None = None,
    end: date | None = None,
    fields: Sequence[str] | None = None,
    allow_remote: bool = True,
    force_refresh: bool | None = None,
    require_fresh_local: bool | None = None,
    allow_partial: bool | None = None,
    ignore_errors: bool = True,
) -> Dict[str, FundamentalFrame]:
    """
    ×˜×•×¢×Ÿ fundamentals ×œ××¡×¤×¨ ××“×“×™× (universe) ×•××—×–×™×¨ dict {symbol -> DataFrame}.

    ×¤×¨××˜×¨×™×
    --------
    symbols : Sequence[str]
        ×¨×©×™××ª ×¡×™××•×œ×™× (××“×“×™×/ETF).
    start, end, fields, allow_remote, force_refresh, require_fresh_local, allow_partial :
        ×¢×•×‘×¨×™× ×œ-load_index_fundamentals.
    ignore_errors : bool
        ×× True â†’ ×× ×™×© ×›×©×œ ×¢×‘×•×¨ ×¡×™××•×œ ××¡×•×™×, × ×¨×©×•× ××–×”×¨×” ×•× ××©×™×š.
        ×× False â†’ × ×–×¨×•×§ ××ª ×”×©×’×™××” ×”×¨××©×•× ×” ×©×ª×•×¤×™×¢.

    ×”×—×–×¨
    -----
    dict[str, DataFrame]
    """
    use_allow_partial = _coerce_allow_partial(allow_partial)
    results: Dict[str, FundamentalFrame] = {}

    for sym in symbols:
        try:
            df = load_index_fundamentals(
                sym,
                start=start,
                end=end,
                fields=fields,
                allow_remote=allow_remote,
                force_refresh=force_refresh,
                require_fresh_local=require_fresh_local,
                allow_partial=use_allow_partial,
            )
            results[_normalize_symbol(sym)] = df
        except Exception as exc:
            if ignore_errors:
                logger.warning(
                    "Failed to load fundamentals for %s: %s (ignore_errors=True)",
                    sym,
                    exc,
                )
                continue
            else:
                raise

    return results


# =========================
# Panel construction (MultiIndex)
# =========================

def _harmonize_columns_for_panel(
    frames: Mapping[str, FundamentalFrame],
) -> Dict[str, FundamentalFrame]:
    """
    ××‘×˜×™×— ×©×›×œ ×”-DataFrame-×™× ×‘-universe ×™×›×™×œ×• ××ª ××•×ª×” ×¨×©×™××ª ×¢××•×“×•×ª,
    ×›×š ×©××¤×©×¨ ×œ×¢×©×•×ª concat ×‘×œ×™ ×”×¤×ª×¢×•×ª.

    - ××–×”×” ××ª ××™×—×•×“ ×›×œ ×”×©×“×•×ª.
    - ×¢×‘×•×¨ ×›×œ ×¡×™××•×œ â†’ ××•×¡×™×£ ×¢××•×“×•×ª ×—×¡×¨×•×ª ×›-NaN.
    """
    if not frames:
        return {}

    all_cols: set[str] = set()
    for df in frames.values():
        all_cols.update(df.columns)

    all_cols_list = sorted(all_cols)
    out: Dict[str, FundamentalFrame] = {}
    for sym, df in frames.items():
        if df.empty:
            # × ×©××•×¨ DataFrame ×¨×™×§ ×¢× ××•×ª×Ÿ ×¢××•×“×•×ª ×œ×›×œ ×”×¤×—×•×ª
            out[sym] = pd.DataFrame(columns=all_cols_list)
            continue

        # reindex columns ×›×“×™ ×œ×”×©×œ×™× ×—×¡×¨×™× ×‘-NaN
        df2 = df.reindex(columns=all_cols_list)
        out[sym] = df2

    return out


def build_fundamentals_panel(
    symbols: Sequence[str],
    *,
    start: date | None = None,
    end: date | None = None,
    fields: Sequence[str] | None = None,
    allow_remote: bool = True,
    force_refresh: bool | None = None,
    require_fresh_local: bool | None = None,
    allow_partial: bool | None = None,
    freq: str | None = None,
    ffill: bool = True,
    ignore_errors: bool = True,
) -> FundamentalPanel:
    """
    ×‘×•× ×” Panel ×©×œ fundamentals ×œ×›×œ ×”-universe ×‘×¤×•×¨××˜ DataFrame ×¢× MultiIndex:

        index  = (date, symbol)
        columns = ×”×©×“×•×ª ×”×¤× ×“×•×× ×˜×œ×™×™×.

    ×™×›×•×œ ×’×:
    - ×œ×‘×¦×¢ resample ×œ×ª×“×™×¨×•×ª ××—×™×“×” (freq="M"/"Q"/"D"/...).
    - ×œ×‘×¦×¢ forward-fill (ffill=True) ×›×“×™ "×œ××¨×•×—" × ×ª×•× ×™× ×¨×‘×¢×•× ×™×™× ×œ×—×•×“×©×™×.

    ×¤×¨××˜×¨×™×
    --------
    symbols : Sequence[str]
        ×¨×©×™××ª ×¡×™××•×œ×™× (××“×“×™×/ETF).
    start, end, fields, allow_remote, force_refresh, require_fresh_local,
    allow_partial, ignore_errors :
        ×¢×•×‘×¨×™× ×œ-load_indices_fundamentals.
    freq : str | None
        ×ª×“×™×¨×•×ª ×œ×¤×× ×œ (×œ××©×œ "M" ×œ×—×•×“×©×™, "Q" ×œ×¨×‘×¢×•× ×™).
        ×× None â†’ ×œ× ××‘×•×¦×¢ resample (××©××™×¨×™× ××ª ××™× ×“×§×¡ ×”× ×ª×•× ×™× ×›×¤×™ ×©×”×•×).
        ×× None ×•-SETTINGS.FUNDAMENTALS_PANEL_FREQ ×œ× None â†’ ××¤×©×¨ ×œ×‘×—×•×¨ ×œ×”×©×ª××©
        ×‘×‘×¨×™×¨×ª ×”××—×“×œ ×××‘× ×” ×”×”×’×“×¨×•×ª ×‘×¢×ª×™×“ (×›×¨×’×¢ freq ×× ×¦×— ×× ×¡×•×¤×§).
    ffill : bool
        ×× True â†’ ×œ××—×¨ resample, × ×‘×¦×¢ forward-fill ×‘×ª×•×š ×›×œ ×¡×™××•×œ.
        ××ª××™× ×œ× ×ª×•× ×™× ×›××• EPS/Book/ROE ×©×”× ×‘×“×¨×š ×›×œ×œ "× ×ª×•× ×™× ×ª×§×•×¤×ª×™×™×"
        ×©××ª×¢×“×›× ×™× ××—×ª ×œ×¨×‘×¢×•×Ÿ/×©× ×”.

    ×”×—×–×¨
    -----
    FundamentalPanel (DataFrame):
        index: MultiIndex (date, symbol)
        columns: ×××•×—×“×™× ×œ×›×œ ×”-universe.
    """
    if not symbols:
        return pd.DataFrame()

    use_allow_partial = _coerce_allow_partial(allow_partial)
    sym_list = list(symbols)

    logger.info(
        "Building fundamentals panel for universe=%s, start=%s, end=%s, "
        "fields=%s, freq=%s, ffill=%s",
        sym_list,
        start,
        end,
        fields,
        freq or SETTINGS.FUNDAMENTALS_PANEL_FREQ,
        ffill,
    )

    # ×©×œ×‘ 1: ×˜×¢×™× ×ª DataFrame ×œ×›×œ ×¡×™××•×œ
    frames_raw = load_indices_fundamentals(
        sym_list,
        start=start,
        end=end,
        fields=fields,
        allow_remote=allow_remote,
        force_refresh=force_refresh,
        require_fresh_local=require_fresh_local,
        allow_partial=use_allow_partial,
        ignore_errors=ignore_errors,
    )

    if not frames_raw:
        return pd.DataFrame()

    # ×©×œ×‘ 2: ××—×™×“×•×ª ×¢××•×“×•×ª
    frames = _harmonize_columns_for_panel(frames_raw)

    # ×©×œ×‘ 3: resample (×× ×‘×™×§×©×• freq)
    panel_pieces: list[pd.DataFrame] = []
    effective_freq = freq or SETTINGS.FUNDAMENTALS_PANEL_FREQ

    for sym, df in frames.items():
        if df.empty:
            continue

        df_sym = df.copy()

        # ×¨×§ ×× ×™×© ××™× ×“×§×¡ ×–××Ÿ
        if isinstance(df_sym.index, pd.DatetimeIndex) and effective_freq:
            try:
                # ×œ×•×§×—×™× ××ª ×”×¢×¨×š ×”××—×¨×•×Ÿ ×‘×›×œ bucket (×¡×‘×™×¨ ×œ×¤× ×“×•×× ×˜×œ)
                df_sym = df_sym.resample(effective_freq).last()
                if ffill:
                    df_sym = df_sym.ffill()
            except Exception as exc:  # pragma: no cover
                logger.warning(
                    "Resample failed for %s with freq=%s: %s",
                    sym,
                    effective_freq,
                    exc,
                )

        df_sym["symbol"] = _normalize_symbol(sym)
        panel_pieces.append(df_sym)

    if not panel_pieces:
        return pd.DataFrame()

    panel = pd.concat(panel_pieces)

    # ×‘× ×™×™×ª MultiIndex (date, symbol)
    if "symbol" in panel.columns:
        panel = panel.reset_index()  # "index" -> date (××• ×©× ××—×¨)
        # × × ×¡×” ×œ×–×”×•×ª ×¢××•×“×ª ×–××Ÿ
        date_col = _guess_date_column(panel) or "index"
        if date_col not in panel.columns:
            # ×× ×–×” ×”××¦×‘, ××©×”×• ×××•×“ ×œ× ×¨×’×™×œ ×§×¨×” â€“ ××‘×œ × ×™×¤×•×œ ×—×–×¨×” ×œ×˜×™×¤×•×œ ××™× ×™××œ×™
            logger.warning(
                "Failed to infer date column when building panel; "
                "result will not have proper MultiIndex."
            )
            return panel  # × ×—×–×™×¨ as-is

        panel = panel.rename(columns={date_col: "date"})
        panel = panel.set_index(["date", "symbol"])
        panel.index = panel.index.set_names(["date", "symbol"])
        # × ×•×•×“× ××™× ×“×§×¡ ×–××Ÿ
        panel = panel.sort_index()

    return panel


# ============================================================
# Part 4/4 â€” Advanced utilities, health checks & integrations
# ============================================================
"""
×‘×—×œ×§ ×”××—×¨×•×Ÿ ×× ×—× ×• ××•×¡×™×¤×™× ×©×›×‘×ª ×›×œ×™× ××ª×§×“××™× ×¡×‘×™×‘ ×©×›×‘×ª ×”×¤× ×“×•×× ×˜×œ:

10 ×¨×¢×™×•× ×•×ª/×™×›×•×œ×•×ª ××¨×›×–×™×•×ª ×©×›×‘×¨ ×ª×›× × ×•:

1. incremental_refresh_index_fundamentals
   - ×¨×¢× ×•×Ÿ ××™× ×§×¨×× ×˜×œ×™ ×œ××“×“ ×‘×•×“×“ (×¨×§ ×-last_date ×•××™×œ×š).

2. refresh_universe_fundamentals
   - ×¨×¢× ×•×Ÿ ××™× ×§×¨×× ×˜×œ×™/××œ× ×œ-universe ×©×œ×.

3. summarize_fundamentals_coverage
   - ×¡×™×›×•× ×›×™×¡×•×™ ×¤× ×“×•×× ×˜×œ×™ (rows/dates/meta) ×œ×›×œ ×¡×™××•×œ.

4. validate_fundamentals_schema
   - ×‘×“×™×§×ª schema â€“ ×”×× ×›×œ ×”×©×“×•×ª ×”×§×¨×™×˜×™×™× ×§×™×™××™×? ×—×¡×¨×™×? ××¡×•×’×™× × ×›×•× ×™×?

5. detect_fundamental_anomalies
   - ×–×™×”×•×™ ×× ×•××œ×™×•×ª ×¤×©×•×˜×•×ª (×§×¤×™×¦×•×ª ×§×™×¦×•× ×™×•×ª, ×¢×¨×›×™× ×‘×œ×ª×™ ×¡×‘×™×¨×™×).

6. fundamentals_health_check
   - "×“×•×— ×‘×¨×™××•×ª" ×œ×“××˜×” â€“ ×¢×“×›× ×™×•×ª, ×›×™×¡×•×™, ×× ×•××œ×™×•×ª.

7. get_latest_fundamentals_snapshot
   - snapshot ×—×“-× ×§×•×“×ª×™ (×ª××¨×™×š ××—×¨×•×Ÿ) ×œ×›×œ ×”×©×“×•×ª ×¢×‘×•×¨ symbol/Universe.

8. fundamentals_panel_to_long
   - ×”××¨×ª Panel MultiIndex ×œ×¤×•×¨××˜ long (date, symbol, field, value).

9. write_fundamentals_panel_to_duckdb
   - ×›×ª×™×‘×ª Panel ×œ-DuckDB ×œ×˜×•×‘×ª ××—×§×¨/×©××™×œ×ª×•×ª ×›×‘×“×•×ª.

10. load_fundamentals_panel_from_duckdb
    - ×˜×¢×™× ×ª Panel ×-DuckDB ×›-DataFrame MultiIndex.

×‘× ×•×¡×£ ××ª×” ×‘×™×§×©×ª **×¢×•×“ 6 ×¨×¢×™×•× ×•×ª ×—×“×©×™×**, ××– ×”×•×¡×¤× ×•:

11. compute_fundamentals_coverage_stats
    - ×¡×˜×˜×™×¡×˜×™×§×•×ª high-level ×¢×œ ×”×›×™×¡×•×™ (×›××” ×¡×™××•×œ×™×, ×××•×¦×¢ rows, ×˜×•×•×— ×ª××¨×™×›×™×).

12. compare_fundamentals_between_universes
    - ×”×©×•×•××ª ×××•×¦×¢×™×/×—×¦×™×•×Ÿ ×©×œ fundamentals ×‘×™×Ÿ ×©× ×™ ×™×§×•××™× (Universe A vs Universe B).

13. prune_stale_fundamentals_files
    - × ×™×§×•×™ ×§×‘×¦×™× ×™×©× ×™×/×œ× ×‘×©×™××•×© â€“ Housekeeping ×‘×¨××ª ××¢×¨×›×ª.

14. export_latest_fundamentals_to_dict
    - ×”×—×–×¨×ª snapshot ××—×¨×•×Ÿ ×›- dict × ×•×— ×œ-UI / JSON.

15. ensure_minimum_history
    - ×‘×“×™×§×” ×©×›×œ ×¡×™××•×œ ×¢×•××“ ×‘×“×¨×™×©×ª ×”×™×¡×˜×•×¨×™×” ××™× ×™××œ×™×ª (×œ××©×œ 10 ×©× ×™× ××—×•×¨×”).

16. generate_fundamentals_markdown_report
    - ×™×¦×™×¨×ª ×“×•×— Markdown ××•×˜×•××˜×™ ×œ×¡×§×™×¨×”/×ª×¦×•×’×” ×‘-UI/×œ×•×’.

×©×•× ×“×‘×¨ ×›××Ÿ ×œ× ××©× ×” ××ª ×”-API ×”×‘×¡×™×¡×™ â€“ ×”×›×œ ×”×¨×—×‘×•×ª ×¡×‘×™×‘×•.
"""

from typing import Optional

# DuckDB ××•×¤×¦×™×•× ×œ×™ â€“ ×‘×¡×‘×™×‘×” ×©××™×Ÿ, ×¤×©×•×˜ × ×“×œ×’ ×¢×œ ×”×¤×•× ×§×¦×™×•×ª ×©××©×ª××©×•×ª ×‘×•
try:  # pragma: no cover
    import duckdb  # type: ignore
    _HAS_DUCKDB = True
except Exception:  # pragma: no cover
    duckdb = None  # type: ignore
    _HAS_DUCKDB = False


# =========================
# 1) Incremental refresh (single symbol)
# =========================

def incremental_refresh_index_fundamentals(
    symbol: str,
    *,
    provider: str | None = None,
    fields: Sequence[str] | None = None,
    allow_partial: bool | None = None,
) -> FundamentalFrame:
    """
    ××‘×¦×¢ ×¨×¢× ×•×Ÿ ××™× ×§×¨×× ×˜×œ×™ ×œ××“×“ ×‘×•×“×“:

    ×œ×•×’×™×§×”:
    -------
    1. ×× ×¡×” ×œ×˜×¢×•×Ÿ ×§×•×‘×¥ ×§×™×™× ××”×“×™×¡×§ (×’× ×× ×œ× ×˜×¨×™).
    2. ××–×”×” ××ª ×”×ª××¨×™×š ×”××—×¨×•×Ÿ ×©×™×© ×œ× ×• (last_date).
    3. ×§×•×¨× ×œ-provider ×”×—×œ ××™×•× ××—×¨×™ last_date (××• ××ª×—×™×œ×ª ×”×™×¡×˜×•×¨×™×” ×× ××™×Ÿ × ×ª×•× ×™×).
    4. ×××—×“ (concat) ×‘×™×Ÿ ×”×™×©×Ÿ ×œ×—×“×©, ×× ×§×” ×›×¤×™×œ×•×™×•×ª, ×× ×¨××œ ×•×©×•××¨ ×œ×“×™×¡×§.
    5. ××—×–×™×¨ ××ª ×”-DataFrame ×”××œ× (×™×©×Ÿ+×—×“×©).
    """
    sym_norm = _normalize_symbol(symbol)
    use_allow_partial = _coerce_allow_partial(allow_partial)

    # 1) ×“××˜×” ×§×™×™×
    df_existing = _load_from_disk(sym_norm, require_fresh=False)

    last_date: date | None = None
    if df_existing is not None and not df_existing.empty:
        if isinstance(df_existing.index, pd.DatetimeIndex):
            last_ts = df_existing.index.max()
            last_date = last_ts.date()

    # 2) ×ª×—×™×œ×ª ×¨×¢× ×•×Ÿ
    if last_date is not None:
        start_refresh = last_date + timedelta(days=1)
    else:
        start_refresh = None

    logger.info(
        "Incremental refresh for %s starting from %s (prev_last_date=%s)",
        sym_norm,
        start_refresh,
        last_date,
    )

    # 3) ××©×™×›×ª ×“××˜×” ×—×“×© ××”-provider
    df_new, effective_provider = _fetch_via_provider(
        sym_norm,
        provider_name=provider,
        start=start_refresh,
        end=None,
        fields=fields,
    )

    df_new = _normalize_fundamentals_df(df_new)

    # ×× ××™×Ÿ ×—×“×© â€“ × ×—×–×™×¨ ××ª ×”×§×™×™×
    if df_existing is not None and not df_existing.empty:
        if df_new is None or df_new.empty:
            logger.info(
                "No new fundamentals found for %s; returning existing data only.",
                sym_norm,
            )
            df_existing = _select_fields(
                df_existing,
                fields,
                allow_partial=use_allow_partial,
                symbol=sym_norm,
            )
            return df_existing

        df_combined = pd.concat([df_existing, df_new])
    else:
        df_combined = df_new

    df_combined = _normalize_fundamentals_df(df_combined)
    df_combined = _select_fields(
        df_combined,
        fields,
        allow_partial=use_allow_partial,
        symbol=sym_norm,
    )

    _save_to_disk(
        sym_norm,
        df_combined,
        source=effective_provider,
        extra_meta={
            "incremental": True,
            "prev_last_date": last_date.isoformat() if last_date else None,
        },
    )

    logger.info(
        "Completed incremental refresh for %s (rows=%s, cols=%s)",
        sym_norm,
        df_combined.shape[0],
        df_combined.shape[1],
    )

    return df_combined


# =========================
# 2) Incremental refresh for universe
# =========================

def refresh_universe_fundamentals(
    symbols: Sequence[str],
    *,
    provider: str | None = None,
    fields: Sequence[str] | None = None,
    allow_partial: bool | None = None,
    ignore_errors: bool = True,
) -> Dict[str, FundamentalFrame]:
    """
    ×¨×¢× ×•×Ÿ ××™× ×§×¨×× ×˜×œ×™ (××• ××œ×, ×× ××™×Ÿ ×“××˜×”) ×œ-universe ×©×œ×.

    ×¢×‘×•×¨ ×›×œ ×¡×™××•×œ:
    - ×§×•×¨× ×œ-incremental_refresh_index_fundamentals.
    - ××—×–×™×¨ dict {symbol -> DataFrame}.
    """
    results: Dict[str, FundamentalFrame] = {}
    sym_list = list(symbols)
    if not sym_list:
        return results

    logger.info(
        "Refreshing universe fundamentals incrementally for %s symbols: %s",
        len(sym_list),
        sym_list,
    )

    for sym in sym_list:
        try:
            df = incremental_refresh_index_fundamentals(
                sym,
                provider=provider,
                fields=fields,
                allow_partial=allow_partial,
            )
            results[_normalize_symbol(sym)] = df
        except Exception as exc:
            if ignore_errors:
                logger.warning(
                    "Failed incremental refresh for %s: %s (ignore_errors=True)",
                    sym,
                    exc,
                )
                continue
            else:
                raise

    return results


# =========================
# 3) Coverage summary & stats
# =========================

def summarize_fundamentals_coverage(
    symbols: Sequence[str] | None = None,
) -> pd.DataFrame:
    """
    ××—×–×™×¨ DataFrame ×¢× ×¡×™×›×•× ×›×™×¡×•×™ ×”×¤× ×“×•×× ×˜×œ ×œ×›×œ ×¡×™××•×œ:

    ×¢××•×“×•×ª:
        symbol
        has_file
        n_rows
        n_cols
        min_date
        max_date
        last_refresh
        fields (comma-separated string)
    """
    if symbols is None:
        symbols = list_cached_symbols()

    rows: list[dict[str, Any]] = []
    for sym in symbols:
        sym_norm = _normalize_symbol(sym)
        meta = _load_metadata(sym_norm)
        df = _load_from_disk(sym_norm, require_fresh=False)

        has_file = df is not None and not df.empty
        n_rows = int(df.shape[0]) if has_file else 0
        n_cols = int(df.shape[1]) if has_file else 0

        min_date = None
        max_date = None
        if has_file and isinstance(df.index, pd.DatetimeIndex):
            try:
                min_date = df.index.min().date().isoformat()
                max_date = df.index.max().date().isoformat()
            except Exception:  # pragma: no cover
                min_date = None
                max_date = None

        last_refresh = None
        fields_list: list[str] = []
        if meta is not None:
            last_refresh = meta.last_refresh.isoformat()
            fields_list = list(meta.fields or [])
        elif has_file:
            fields_list = list(df.columns)

        rows.append(
            {
                "symbol": sym_norm,
                "has_file": has_file,
                "n_rows": n_rows,
                "n_cols": n_cols,
                "min_date": min_date,
                "max_date": max_date,
                "last_refresh": last_refresh,
                "fields": ", ".join(sorted(set(fields_list))) if fields_list else None,
            }
        )

    cov_df = pd.DataFrame(rows)
    cov_df = cov_df.sort_values("symbol").reset_index(drop=True)
    return cov_df


def compute_fundamentals_coverage_stats(
    symbols: Sequence[str] | None = None,
) -> Dict[str, Any]:
    """
    ×¡×˜×˜×™×¡×˜×™×§×•×ª high-level ×¢×œ ×›×™×¡×•×™ fundamentals:

    ××—×–×™×¨ dict ×¢×:
        n_symbols
        n_with_files
        avg_rows
        median_rows
        min_start_date
        max_end_date
    """
    cov = summarize_fundamentals_coverage(symbols)
    if cov.empty:
        return {
            "n_symbols": 0,
            "n_with_files": 0,
            "avg_rows": 0.0,
            "median_rows": 0.0,
            "min_start_date": None,
            "max_end_date": None,
        }

    with_files = cov[cov["has_file"]]

    def _safe_min(col: str) -> Optional[str]:
        vals = [v for v in with_files[col].dropna().tolist() if v]
        return min(vals) if vals else None

    def _safe_max(col: str) -> Optional[str]:
        vals = [v for v in with_files[col].dropna().tolist() if v]
        return max(vals) if vals else None

    stats = {
        "n_symbols": int(len(cov)),
        "n_with_files": int(with_files.shape[0]),
        "avg_rows": float(with_files["n_rows"].mean()) if not with_files.empty else 0.0,
        "median_rows": float(with_files["n_rows"].median()) if not with_files.empty else 0.0,
        "min_start_date": _safe_min("min_date"),
        "max_end_date": _safe_max("max_date"),
    }
    return stats


# =========================
# 4) Schema validation & anomalies
# =========================

def validate_fundamentals_schema(
    df: FundamentalFrame,
    required_fields: Sequence[str],
    *,
    symbol: str | None = None,
) -> Dict[str, Any]:
    """
    ×‘×•×“×§ ×”×× DataFrame ×©×œ fundamentals ××›×™×œ ××ª ×›×œ ×”×©×“×•×ª ×”× ×“×¨×©×™×.

    ××—×–×™×¨ dict:
        {
            "ok": bool,
            "missing": [list of missing fields],
            "present": [list of present fields]
        }
    """
    df_cols_lower = {c.lower() for c in df.columns}
    requested_lower = [str(f).lower().strip() for f in required_fields]

    missing = [f for f in requested_lower if f not in df_cols_lower]
    present = [f for f in requested_lower if f in df_cols_lower]

    ok = len(missing) == 0
    if not ok:
        logger.warning(
            "Fundamentals schema validation failed for %s: missing=%s",
            symbol or "<unknown>",
            missing,
        )

    return {
        "ok": ok,
        "missing": missing,
        "present": present,
    }


def detect_fundamental_anomalies(
    df: FundamentalFrame,
    *,
    symbol: str | None = None,
    numeric_fields: Sequence[str] | None = None,
    zscore_threshold: float = 6.0,
) -> pd.DataFrame:
    """
    ××–×”×” ×× ×•××œ×™×•×ª ×‘×¡×™×¡×™×•×ª ×‘×¤× ×“×•×× ×˜×œ:

    ×›×œ×œ×™×:
    -------
    - ×× numeric_fields=None â†’ × ×©×ª××© ×‘×›×œ ×”×¢××•×“×•×ª ×”××¡×¤×¨×™×•×ª.
    - × ×—×©×‘ Z-score ×œ×›×œ ×¡×“×¨×” (field) ×•× ×–×”×” ×¢×¨×›×™× ×¢× |z| > zscore_threshold.
    - ×‘× ×•×¡×£, ×¢×‘×•×¨ ×¤×™×¦'×¨×™× ×©×œ× ×××•×¨×™× ×œ×”×™×•×ª ×©×œ×™×œ×™×™× (×›××• pe, pb, dividend_yield),
      × ×–×”×” ×¢×¨×›×™× ×©×œ×™×œ×™×™×.

    ××—×–×™×¨ DataFrame ×©×œ anomalies:
        columns: date, field, value, zscore, rule
    """
    if df.empty:
        return pd.DataFrame(columns=["date", "field", "value", "zscore", "rule"])

    if not isinstance(df.index, pd.DatetimeIndex):
        logger.warning(
            "detect_fundamental_anomalies: DataFrame index is not DatetimeIndex "
            "for %s; anomaly detection may be limited.",
            symbol or "<unknown>",
        )

    # ×‘×—×™×¨×ª ×©×“×•×ª ××¡×¤×¨×™×™×
    if numeric_fields is None:
        num_df = df.select_dtypes(include=["number"])
        numeric_fields = list(num_df.columns)

    records: list[dict[str, Any]] = []

    for field in numeric_fields:
        series = df.get(field)
        if series is None:
            continue
        try:
            s = pd.to_numeric(series, errors="coerce").dropna()
        except Exception:  # pragma: no cover
            continue
        if s.empty:
            continue

        mean = s.mean()
        std = s.std(ddof=0) or 1e-9
        zscores = (s - mean) / std

        # ×›×œ×œ 1: |z| ×’×“×•×œ
        mask_extreme = zscores.abs() > zscore_threshold
        for ts, z in zscores[mask_extreme].items():
            records.append(
                {
                    "date": ts,
                    "field": field,
                    "value": s.loc[ts],
                    "zscore": float(z),
                    "rule": f"|z|>{zscore_threshold}",
                }
            )

        # ×›×œ×œ 2: ×©×œ×™×œ×™ ×œ×¤×™×¦'×¨×™× ×©×œ× ×××•×¨×™× ×œ×”×™×•×ª ×©×œ×™×œ×™×™×
        non_negative_candidates = ("pe", "pb", "dividend", "yield", "margin")
        if any(tok in field for tok in non_negative_candidates):
            negative_mask = s < 0
            for ts, val in s[negative_mask].items():
                records.append(
                    {
                        "date": ts,
                        "field": field,
                        "value": float(val),
                        "zscore": None,
                        "rule": "negative_for_non_negative_field",
                    }
                )

    if not records:
        return pd.DataFrame(columns=["date", "field", "value", "zscore", "rule"])

    anomalies = pd.DataFrame(records)
    anomalies = anomalies.sort_values(["date", "field"]).reset_index(drop=True)
    return anomalies


# =========================
# 5) Health check
# =========================

def fundamentals_health_check(
    symbols: Sequence[str] | None = None,
    *,
    required_fields: Sequence[str] | None = None,
) -> Dict[str, Any]:
    """
    "×“×•×— ×‘×¨×™××•×ª" ×œ×“××˜×” ×”×¤× ×“×•×× ×˜×œ×™:

    ××—×–×™×¨ dict:
        {
            "coverage": DataFrame (summarize_fundamentals_coverage),
            "coverage_stats": dict (compute_fundamentals_coverage_stats),
            "schema_issues": dict[symbol -> {missing/present}],
            "anomaly_counts": dict[symbol -> int]
        }
    """
    if symbols is None:
        symbols = list_cached_symbols()

    coverage = summarize_fundamentals_coverage(symbols)
    coverage_stats = compute_fundamentals_coverage_stats(symbols)

    schema_issues: Dict[str, Any] = {}
    anomaly_counts: Dict[str, int] = {}

    for sym in symbols:
        sym_norm = _normalize_symbol(sym)
        df = _load_from_disk(sym_norm, require_fresh=False)
        if df is None or df.empty:
            continue

        if required_fields:
            schema_issues[sym_norm] = validate_fundamentals_schema(
                df,
                required_fields=required_fields,
                symbol=sym_norm,
            )

        anomalies = detect_fundamental_anomalies(df, symbol=sym_norm)
        anomaly_counts[sym_norm] = int(anomalies.shape[0])

    return {
        "coverage": coverage,
        "coverage_stats": coverage_stats,
        "schema_issues": schema_issues,
        "anomaly_counts": anomaly_counts,
    }


# =========================
# 6) Snapshots & export
# =========================

def get_latest_fundamentals_snapshot(
    symbol_or_symbols: str | Sequence[str],
    *,
    fields: Sequence[str] | None = None,
    allow_remote: bool = True,
) -> pd.DataFrame:
    """
    ××—×–×™×¨ snapshot ××—×¨×•×Ÿ (×ª××¨×™×š ××—×¨×•×Ÿ) ×¢×‘×•×¨ symbol ××—×“ ××• ×¨×©×™××ª symbols.

    ×”×—×–×¨:
    ------
    DataFrame ×¢×:
        index: symbol
        columns: fields (×× ×¦×•×™×Ÿ) ××• ×›×œ ×”×¢××•×“×•×ª.

    ×”×¢×¨×”:
    ------
    ×œ× ××—×–×™×¨ ××ª ×¢××•×“×ª ×”×ª××¨×™×š â€“ ×–×• ×›×‘×¨ "×—×ª×™×›×ª ××¦×‘" ××—×¨×•× ×”.
    """
    if isinstance(symbol_or_symbols, str):
        symbols = [symbol_or_symbols]
    else:
        symbols = list(symbol_or_symbols)

    rows: list[pd.Series] = []
    index: list[str] = []

    for sym in symbols:
        df = load_index_fundamentals(
            sym,
            start=None,
            end=None,
            fields=fields,
            allow_remote=allow_remote,
            force_refresh=False,
            require_fresh_local=False,
            allow_partial=True,
        )
        if df is None or df.empty:
            continue

        if isinstance(df.index, pd.DatetimeIndex):
            last_row = df.iloc[-1]
        else:
            last_row = df.iloc[-1]

        rows.append(last_row)
        index.append(_normalize_symbol(sym))

    if not rows:
        return pd.DataFrame()

    snap_df = pd.DataFrame(rows, index=index)
    snap_df.index.name = "symbol"
    return snap_df


def export_latest_fundamentals_to_dict(
    symbol_or_symbols: str | Sequence[str],
    *,
    fields: Sequence[str] | None = None,
    allow_remote: bool = True,
) -> Dict[str, Dict[str, Any]]:
    """
    ××—×–×™×¨ snapshot ××—×¨×•×Ÿ ×›- dict × ×•×— ×œ-UI / JSON:

        {
            "SPY": {"pe": 27.3, "pb": 4.9, ...},
            "QQQ": {...},
            ...
        }
    """
    snap_df = get_latest_fundamentals_snapshot(
        symbol_or_symbols,
        fields=fields,
        allow_remote=allow_remote,
    )
    if snap_df.empty:
        return {}

    out: Dict[str, Dict[str, Any]] = {}
    for sym, row in snap_df.iterrows():
        out[str(sym)] = {k: (None if pd.isna(v) else v) for k, v in row.items()}
    return out


# =========================
# 7) Panel transformations
# =========================

def fundamentals_panel_to_long(
    panel: FundamentalPanel,
) -> pd.DataFrame:
    """
    ×××™×¨ Panel MultiIndex (date, symbol) ×œ×¤×•×¨××˜ long:

        columns: ["date", "symbol", "field", "value"]

    ×©×™××•×©×™ ×œ-analytics / plotting / ×©××™×¨×” ×›-CSV/DB.
    """
    if panel is None or panel.empty:
        return pd.DataFrame(columns=["date", "symbol", "field", "value"])

    if not isinstance(panel.index, pd.MultiIndex) or panel.index.names != ["date", "symbol"]:
        logger.warning(
            "fundamentals_panel_to_long: panel index is not MultiIndex ['date', 'symbol']; attempting best-effort reshape."
        )

    long_df = panel.reset_index().melt(
        id_vars=["date", "symbol"],
        var_name="field",
        value_name="value",
    )
    return long_df


# =========================
# 8) DuckDB integration (optional)
# =========================

def write_fundamentals_panel_to_duckdb(
    panel: FundamentalPanel,
    *,
    db_path: str | Path,
    table_name: str = "fundamentals_panel",
    mode: str = "replace",
) -> None:
    """
    ×›×•×ª×‘ Panel ×œ-DuckDB (×× ××•×ª×§×Ÿ):

    ×¤×¨××˜×¨×™×:
        db_path   : ×”× ×ª×™×‘ ×œ×§×•×‘×¥ DuckDB (×œ××©×œ "data/fundamentals.duckdb").
        table_name: ×©× ×”×˜×‘×œ×”.
        mode      : "replace" | "append".
    """
    if not _HAS_DUCKDB:
        logger.warning(
            "write_fundamentals_panel_to_duckdb: duckdb not installed; skipping."
        )
        return

    if panel is None or panel.empty:
        logger.info("write_fundamentals_panel_to_duckdb: panel is empty; nothing to write.")
        return

    db_path = Path(db_path)
    long_df = fundamentals_panel_to_long(panel)

    conn = duckdb.connect(str(db_path))
    try:
        if mode == "replace":
            conn.execute(f"DROP TABLE IF EXISTS {table_name}")
        conn.register("tmp_fundamentals", long_df)
        conn.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {table_name} AS
            SELECT * FROM tmp_fundamentals
            """
        )
        if mode == "append":
            conn.execute(
                f"""
                INSERT INTO {table_name}
                SELECT * FROM tmp_fundamentals
                """
            )
        logger.info(
            "Wrote fundamentals panel to DuckDB %s (table=%s, rows=%s)",
            db_path,
            table_name,
            long_df.shape[0],
        )
    finally:
        conn.close()


def load_fundamentals_panel_from_duckdb(
    *,
    db_path: str | Path,
    table_name: str = "fundamentals_panel",
) -> FundamentalPanel:
    """
    ×˜×•×¢×Ÿ Panel ×‘×¡×™×¡×™ ×-DuckDB:

    ××¦×•×¤×” ×©×¤×•×¨××˜ ×”×˜×‘×œ×” ×™×”×™×”:
        [date, symbol, field, value]

    ××—×–×™×¨ DataFrame MultiIndex (date, symbol) ×‘×¢×™×¦×•×‘ "wide".
    """
    if not _HAS_DUCKDB:
        logger.warning(
            "load_fundamentals_panel_from_duckdb: duckdb not installed; returning empty DataFrame."
        )
        return pd.DataFrame()

    db_path = Path(db_path)
    if not db_path.exists():
        logger.warning(
            "load_fundamentals_panel_from_duckdb: db_path %s does not exist.",
            db_path,
        )
        return pd.DataFrame()

    conn = duckdb.connect(str(db_path))
    try:
        df_long = conn.execute(f"SELECT * FROM {table_name}").fetch_df()
    finally:
        conn.close()

    if df_long.empty:
        return pd.DataFrame()

    # ××¦×•×¤×” ×©×™×© ×¢××•×“×•×ª date, symbol, field, value
    required_cols = {"date", "symbol", "field", "value"}
    if not required_cols.issubset(df_long.columns):
        logger.warning(
            "load_fundamentals_panel_from_duckdb: table %s has columns %s, "
            "expected at least %s.",
            table_name,
            list(df_long.columns),
            required_cols,
        )
        return pd.DataFrame()

    df_wide = df_long.pivot_table(
        index=["date", "symbol"],
        columns="field",
        values="value",
    )
    df_wide.columns = [str(c) for c in df_wide.columns]
    df_wide = df_wide.sort_index()

    return df_wide


# =========================
# 9) Universe comparison & history requirements
# =========================

def compare_fundamentals_between_universes(
    universe_a: Sequence[str],
    universe_b: Sequence[str],
    *,
    fields: Sequence[str],
    start: date | None = None,
    end: date | None = None,
) -> pd.DataFrame:
    """
    ×”×©×•×•××ª ×××•×¦×¢×™×/×—×¦×™×•×Ÿ ×©×œ fundamentals ×‘×™×Ÿ ×©× ×™ ×™×§×•××™× (Universe A vs B).

    ××—×–×™×¨ DataFrame ×¢×:
        index: field
        columns: ["mean_a", "mean_b", "diff", "median_a", "median_b"]
    """
    panel_a = build_fundamentals_panel(
        universe_a,
        start=start,
        end=end,
        fields=fields,
        allow_remote=False,
        force_refresh=False,
        require_fresh_local=False,
        allow_partial=True,
        freq=None,
        ffill=False,
        ignore_errors=True,
    )
    panel_b = build_fundamentals_panel(
        universe_b,
        start=start,
        end=end,
        fields=fields,
        allow_remote=False,
        force_refresh=False,
        require_fresh_local=False,
        allow_partial=True,
        freq=None,
        ffill=False,
        ignore_errors=True,
    )

    long_a = fundamentals_panel_to_long(panel_a)
    long_b = fundamentals_panel_to_long(panel_b)

    stats: list[dict[str, Any]] = []
    for field in fields:
        fa = long_a[long_a["field"] == field].dropna(subset=["value"])
        fb = long_b[long_b["field"] == field].dropna(subset=["value"])
        if fa.empty and fb.empty:
            continue

        mean_a = float(fa["value"].mean()) if not fa.empty else float("nan")
        mean_b = float(fb["value"].mean()) if not fb.empty else float("nan")
        median_a = float(fa["value"].median()) if not fa.empty else float("nan")
        median_b = float(fb["value"].median()) if not fb.empty else float("nan")

        stats.append(
            {
                "field": field,
                "mean_a": mean_a,
                "mean_b": mean_b,
                "diff": mean_a - mean_b if pd.notna(mean_a) and pd.notna(mean_b) else float("nan"),
                "median_a": median_a,
                "median_b": median_b,
            }
        )

    if not stats:
        return pd.DataFrame()

    df_stats = pd.DataFrame(stats).set_index("field")
    return df_stats


def ensure_minimum_history(
    symbols: Sequence[str],
    *,
    min_start_date: date,
) -> Dict[str, bool]:
    """
    ×‘×•×“×§ ×©×›×œ ×¡×™××•×œ ×¢×•××“ ×‘×“×¨×™×©×ª ×”×™×¡×˜×•×¨×™×” ××™× ×™××œ×™×ª (×œ××©×œ 10 ×©× ×™× ××—×•×¨×”).

    ××—×–×™×¨ dict {symbol -> bool} ×”×× ×¢×•××“ ×‘×“×¨×™×©×”.
    """
    result: Dict[str, bool] = {}
    for sym in symbols:
        sym_norm = _normalize_symbol(sym)
        df = _load_from_disk(sym_norm, require_fresh=False)
        ok = False
        if df is not None and not df.empty and isinstance(df.index, pd.DatetimeIndex):
            try:
                min_dt = df.index.min().date()
                ok = min_dt <= min_start_date
            except Exception:  # pragma: no cover
                ok = False
        result[sym_norm] = ok
        if not ok:
            logger.warning(
                "Symbol %s does not meet minimum history requirement (min_date>%s).",
                sym_norm,
                min_start_date,
            )
    return result


# =========================
# 10) Housekeeping & reports
# =========================

def prune_stale_fundamentals_files(
    *,
    older_than_days: int,
    dry_run: bool = True,
) -> list[Path]:
    """
    ××•×—×§ (××• ××¡××Ÿ) ×§×‘×¦×™ fundamentals ×™×©× ×™× ××ª×™×§×™×™×ª ×”×“××˜×”.

    ×¤×¨××˜×¨×™×:
        older_than_days : ×§×‘×¦×™× ×©×’×™×œ× ×’×“×•×œ ××›×š ×™×™×—×©×‘×• "×™×©× ×™×".
        dry_run         : ×× True â†’ ×œ× ××•×—×§ ×‘×¤×•×¢×œ, ×¨×§ ××—×–×™×¨ ×¨×©×™××ª ×§×‘×¦×™× ×œ××—×§.

    ×”×—×–×¨:
        ×¨×©×™××ª Paths ×©× ××¦××• ×™×©× ×™× (×•×× dry_run=False â€“ × ××—×§×• ×‘×¤×•×¢×œ).
    """
    base = SETTINGS.FUNDAMENTALS_DATA_DIR
    if not base.exists():
        return []

    cutoff = datetime.now() - timedelta(days=older_than_days)
    candidates: list[Path] = []
    for p in base.iterdir():
        if not p.is_file():
            continue
        if p.suffix.lower() not in (".parquet", ".csv", ".json"):
            continue
        try:
            mtime = datetime.fromtimestamp(p.stat().st_mtime)
        except OSError:  # pragma: no cover
            continue
        if mtime < cutoff:
            candidates.append(p)

    deleted: list[Path] = []
    for p in candidates:
        if dry_run:
            logger.info("[DRY-RUN] Would delete stale fundamentals file: %s", p)
        else:
            try:
                p.unlink()
                logger.info("Deleted stale fundamentals file: %s", p)
                deleted.append(p)
            except Exception as exc:  # pragma: no cover
                logger.warning("Failed to delete stale fundamentals file %s: %s", p, exc)

    return candidates if dry_run else deleted


def generate_fundamentals_markdown_report(
    symbols: Sequence[str] | None = None,
    *,
    required_fields: Sequence[str] | None = None,
) -> str:
    """
    ×™×•×¦×¨ ×“×•×— Markdown ××¡×•×›× ×¢×œ ×›×™×¡×•×™ ×•×‘×¨×™××•×ª ×”×¤× ×“×•×× ×˜×œ.

    ××ª××™× ×œ×”×–×¨×§×” ×œ×˜××‘ "×“×•×— ×ª×—×–×•×§×”" ×‘×“×©×‘×•×¨×“.
    """
    health = fundamentals_health_check(
        symbols=symbols,
        required_fields=required_fields,
    )
    cov: pd.DataFrame = health["coverage"]
    stats: Dict[str, Any] = health["coverage_stats"]
    anomaly_counts: Dict[str, int] = health["anomaly_counts"]

    lines: list[str] = []
    lines.append("# Fundamentals Health Report")
    lines.append("")
    lines.append("## Coverage Stats")
    lines.append("")
    lines.append(f"- Number of symbols tracked: **{stats.get('n_symbols', 0)}**")
    lines.append(f"- Symbols with files: **{stats.get('n_with_files', 0)}**")
    lines.append(f"- Avg rows per symbol: **{stats.get('avg_rows', 0):.1f}**")
    lines.append(f"- Median rows per symbol: **{stats.get('median_rows', 0):.1f}**")
    lines.append(
        f"- Earliest start date: **{stats.get('min_start_date') or 'N/A'}**, "
        f"latest end date: **{stats.get('max_end_date') or 'N/A'}**"
    )
    lines.append("")

    if not cov.empty:
        lines.append("## Per-Symbol Coverage (top 20)")
        lines.append("")
        # × ×™×§×— ×¢×“ 20 ×©×•×¨×•×ª ×œ×¦××¦×•×
        head_cov = cov.head(20).copy()
        # × ×”×¤×•×š ×œ×˜×‘×œ×” Markdown ×‘×¡×™×¡×™×ª
        lines.append(head_cov.to_markdown(index=False))
        lines.append("")

    if anomaly_counts:
        lines.append("## Anomaly Counts")
        lines.append("")
        for sym, cnt in sorted(anomaly_counts.items()):
            lines.append(f"- **{sym}**: {cnt} anomalies")
        lines.append("")

    return "\n".join(lines)


# ============================================================
# Final __all__ â€” covers all 4 parts of fundamental_loader.py
# ============================================================

__all__ = [
    # Settings & Types
    "FundamentalsSettings",
    "SETTINGS",
    "FundamentalFrame",
    "FundamentalPanel",
    "FundamentalMeta",

    # Providers
    "BaseFundamentalsProvider",
    "register_fundamentals_provider",
    "get_fundamentals_provider",

    # Public API ×œ×˜×¢×™× ×” / ×¤×× ×œ
    "load_index_fundamentals",
    "load_index_fundamentals_cached",
    "load_indices_fundamentals",
    "build_fundamentals_panel",

    # Diagnostics ×‘×¡×™×¡×™×™×
    "list_cached_symbols",
    "get_cached_meta",

    # Incremental & Universe refresh
    "incremental_refresh_index_fundamentals",
    "refresh_universe_fundamentals",

    # Coverage & Health
    "summarize_fundamentals_coverage",
    "compute_fundamentals_coverage_stats",
    "validate_fundamentals_schema",
    "detect_fundamental_anomalies",
    "fundamentals_health_check",
    "ensure_minimum_history",

    # Snapshots & Export
    "get_latest_fundamentals_snapshot",
    "export_latest_fundamentals_to_dict",

    # Panel ops & DuckDB
    "fundamentals_panel_to_long",
    "write_fundamentals_panel_to_duckdb",
    "load_fundamentals_panel_from_duckdb",

    # Universe comparison, housekeeping & reports
    "compare_fundamentals_between_universes",
    "prune_stale_fundamentals_files",
    "generate_fundamentals_markdown_report",
]
